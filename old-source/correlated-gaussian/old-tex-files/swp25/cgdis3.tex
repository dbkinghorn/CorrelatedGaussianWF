%% This document created by Scientific Word (R) Version 2.0
%% Starting shell: book


\documentclass[12pt,thmsa,suthesis,verbatim]{report}
\usepackage{amssymb}
%%%%%%
\usepackage{sw20jrep}
\usepackage{suthesis}
\usepackage{verbatim}

\input tcilatex
\QQQ{Language}{
American English
}

\begin{document}

\title{EXPLICITLY CORRELATED GAUSSIAN BASIS FUNCTIONS:\\
DERIVATION AND IMPLEMENTATION OF MATRIX \\
ELEMENTS AND GRADIENT FORMULAS USING \\
MATRIX DIFFERENTIAL CALCULUS}
\author{DONALD B. KINGHORN}

% front matter

% Title Page

\dept{Department of Chemistry}

\submitdate{August 1995}

\copyrightfalse

\figurespagefalse

\beforepreface
\prefacesection{Acknowledgements}

\begin{center}
{\large ACKNOWLEDGMENT}
\end{center}

%  \ \newline

I would like to thank my dissertation advisor and friend, R. D. Poshusta,
for the advice, assistance, and encouragement he generously offered through
all phases of this dissertation project. His careful review lead to many
improvements in the manuscript. Any remaining errors in content or style are
entirely my own.

\prefacesection{Abstract}

\begin{center}
{EXPLICITLY CORRELATED GAUSSIAN BASIS FUNCTIONS:\\DERIVATION AND
IMPLEMENTATION OF MATRIX \\ELEMENTS AND GRADIENT FORMULAS USING \\MATRIX
DIFFERENTIAL CALCULUS}

\textbf{Abstract} \\\ \\by Donald B. Kinghorn, Ph.D. \\[-0.9em]Washington
State University \\[-0.9em]August 1995
\end{center}

\ \newline
{\noindent Chair: R.D. Poshusta} \ \newline

The matrix differential calculus is introduced to the quantum chemistry
community via new matrix derivations of integral formulas and gradients for
Hamiltonian matrix elements in a basis of correlated Gaussian functions.
Requisite mathematical background material on Kronecker products, Hadamard
products, the vec and vech operators, linear structures, and matrix
differential calculus is presented. New matrix forms for the kinetic and
potential energy operators are presented. Integrals for overlap, kinetic
energy and potential energy matrix elements are derived in matrix form using
matrix calculus. The gradient of the energy functional with respect to the
correlated Gaussian exponent matrices is derived. Burdensome summation
notation is entirely replaced with a compact matrix notation that is both
theoretically and computationally insightful.

These new formulas in the basis of explicitly correlated Gaussian basis
functions, are implemented and applied to find variational upper bounds for
non-relativistic ground states of $^4$He, $^\infty $He, Ps$_2$, $^9$Be, and $%
^\infty $Be. Analytic gradients of the energy are included to speed
optimization of the exponential variational parameters. Five different
nonlinear optimization subroutines (algorithms) are compared: TN, truncated
Newton; DUMING, quasi-Newton; DUMIDH, modified Newton; DUMCGG, conjugate
gradient; POWELL, direction set (non-gradient). The new analytic gradient
formulas are found to significantly accelerate optimizations that require
gradients. The truncated Newton algorithm is found to outperform the other
optimizers for the selected test cases. Computer timings and energy bounds
are reported. The new TN bounds surpass previously reported bounds with the
same basis size.

\afterpreface

%\author{The Author}
%\title{The Title }
%\date{The Date }
%\maketitle

%\begin{abstract}
%Replace this text with your own abstract.
%\end{abstract}

\chapter{General Introduction}

Highly accurate quantum mechanical wave functions, capable of true benchmark
quality for few particle systems, can be achieved with explicitly correlated
basis functions. The accuracy obtainable with vast numbers of orbital
product configurations can be obtained easily with a few correlated basis
functions. But extending this approach beyond about 4 particles has resisted
the best efforts of skilled researchers.

Two types of explicitly correlated basis functions have been developed. The
first, and most rapidly convergent, is based on the Hylleraas functions
incorporating exponentials in all inter-particle distances. The second, and
simplest to implement, is based on Boyes' correlated Gaussian functions
incorporating exponentials in the \emph{squares} of all inter-particle
distances. Required integrals involving the Hylleraas basis are so difficult
that this basis has been effectively limited to 2, 3 and 4 particle systems.
The corresponding integrals involving the correlated Gaussian basis are
simple to evaluate for arbitrary numbers of particles. Furthermore, these
latter integrals (which were tedious in previous formulations) have been
greatly simplified by application of the matrix calculus as reported in
Chapter 2 of this work\cite{Kinghorn95a}.

There are two major difficulties in the way of widespread applications of
explicitly correlated Gaussian basis functions: the n! problem of
permutational symmetry, and the optimization problem of many non-linear
variational parameters.

To satisfy the antisymmetry principle for n Fermions the antisymmetrizer
containing n! permutations is applied. Orbital product wave functions
effectively reduce the n! permutations to the order of n$^3$ operations for
Slater determinants. But all, or nearly all, n! permutations must be
explicitly included for correlated basis functions causing computational
effort to grow nearly exponentially with the size of the system.

The severity of the optimization problem can be seen from the example of 128
basis functions for the beryllium atom for which 1280 parameters must be
optimized. Two aspects of the optimization problem are addressed in this
work: analytic gradients of the energy, and choice of optimization strategy.
By exploiting the analytic gradient formulas achieved in Chapter 2,
optimization is accelerated (in any strategy that calls for gradients).
Trials with five diverse optimization strategies (algorithms) allow the TN
(truncated Newton) method to be identified as the best performer among them.

The optimization of exponent parameters in explicitly correlated Gaussian
functions has been a frustrating problem for researchers for the 35 years
since this basis set was introduced. The new compact matrix based formulas
for integrals and derivatives developed in this work have made it possible
to implement highly efficient computer code for the optimization of exponent
parameters. The analytic gradient based optimization method developed has
lead to the best ground state energy results to date using this basis set
for the systems $^4$He, $^\infty $He, Ps$_2$, $^9$Be and $^\infty $Be.

New advances in science are often preceded by new developments in
mathematics. The new integral and derivative formulas presented in this
dissertation were derived using new mathematics, matrix differential
calculus.

In 1988 Magnus and Neudecker presented this mathematics in a book titled 
\emph{Matrix Differential Calculus with Applications in Statistics and
Econometrics }\cite{MagNeud88}. As the second part of the title suggests,
most of the application of this mathematics has been in the field of
mathematical economics. The two papers comprising this dissertation are the
first works in the chemical literature to utilize this matrix differential
calculus.

The first paper\cite{Kinghorn95a}, presented in Chapter 2, gives a
description of matrix differential calculus and other advanced matrix
techniques. This powerful mathematics is then utilized to derive
matrix/vector operators for kinetic and potential energy, compact integral
formulas for matrix elements of explicitly correlated Gaussians and finally
a complete analytic gradient of the energy functional in this basis. Aside
from the significant results for explicitly correlated Gaussian basis
functions that are presented, it is hoped that this paper will serve as
inspiration for other applications of these powerful matrix techniques.

The second paper\cite{Kinghorn95b}, presented in Chapter 3, fleshes out the
theory developed in the preceding paper. The new formulas are implemented in
computer code and tested with very satisfying results. This second paper
will be of immediate interest to researchers working with explicitly
correlated Gaussian basis functions and will make possible many new
computational results.

The final appendices contain the Fortran subroutines for the energy and
gradient computation. Example input files for the test systems and
parameters for wave functions with up to 16 basis functions are also
included in the appendices.

\chapter{Integrals and Derivatives for Correlated Gaussian Functions using
Matrix Differential Calculus}

\newpage\ 

\section{Introduction}

Most problems in quantum chemistry can be formulated in matrix form and
often involve variational principles and hence matrix derivatives (sometimes
with respect to matrix variables). Without an effective matrix calculus,
differentiation of matrix equations leads to torturous masses of summation
signs and indices which are tedious to verify, non-insightful and not
optimally designed for high performance computing. On the other hand, if
derivatives are written in matrix form all of the tools of matrix algebra
can be applied, and the advantages of matrix notation can be exploited for
new theoretical and computational insight. This paper address these issues
and illustrates them for a long-standing quantum chemical basis set.

An elegant and very useful form of matrix differential calculus has been
presented by the mathematical economists J. R. Magnus and H. Neudecker\cite
{MagNeud88}. This powerful mathematical tool is not well known outside of
the disciplines of mathematical statistics and econometrics. A Citation
Index search indicates that this is the first paper in the chemical
literature to reference the matrix differential calculus of the above
authors.

Matrix differential calculus is a logical extension of matrix algebra, that
preserves the matrix form of matrix problems even after differentiation with
respect to matrix variables. Analytic gradients can be computed with
relative ease using matrix calculus, thus facilitating optimization of
matrix equations with matrix variables. A more subtle, but perhaps more
important, consideration is that compact matrix notation often facilitates
new theoretical insight. Furthermore, most modern high performance computing
machinery utilizes highly optimized libraries of matrix algorithms to
achieve peak performance. If a problem can be written, and kept, in matrix
form there are theoretical and computational advantages in doing so.

The utility of the matrix differential calculus is well illustrated by
considering correlated Gaussian basis functions. Since they were introduced
by Boys\cite{Boys} and Singer\cite{Singer1}, these basis functions have
frequently been used for highly accurate variational calculations on few
particle systems. A recent review cites numerous references\cite{Kozlowski93}
and additional applications continue to appear\cite{Kinghorn93,Zhang94}.
Derivations of integrals and gradient formulas on this basis are typically
encumbered by summation signs and multiply sub- and super-scripted variables
(a notable exception is the paper by Poshusta\cite{Poshusta78}). Since
correlated Gaussian basis functions are matrix functions and derivatives are
intrinsic to deriving their quantum chemical matrix elements, the matrix
calculus is the appropriate tool for dealing with them.

New matrix formulations are given in this paper for kinetic and potential
energy operators, integrals and derivatives. Section~2 introduces the
required notation and definitions for Kronecker products, Hadamard products,
the vec and vech operator, linear structures and matrix differential
calculus. Section~3 displays correlated Gaussian functions as matrix
functions. Section~4 defines and derives matrix forms for the overlap,
kinetic energy and potential energy integrals. This section presents a new
form for the many particle kinetic energy operator, writing it as an
inner-product of gradient vector operators and introduces two new linear
structures which are used to write the potential energy operator in matrix
form. The total kinetic energy integral is derived using matrix calculus and
the potential energy integral is evaluated and displayed in matrix form.
Section~5 gives the gradient of the energy functional at the core of the
variational method and the optimization task. First derivatives required for
the gradient are derived in section~6. Note: The correctness of all integral
formulas and derivatives has been confirmed numerically.

\section{Preliminary Matrix Results}

Certain preliminary definitions and properties are required in order to
place the matrix calculus on a firm foundation. The first essential concept
is the vec operator that sends a matrix into a sequentially ordered set of
its elements (as a column vector); these elements become the independent
variables in terms of which matrix derivatives are defined. For matrices
with linear relations among their elements, additional operators (vech,
basis matrices, etc.) are defined for the purpose of extracting only the
linearly independent variables. We begin with notational conventions and
some needed results concerning Kronecker and Hadamard products.

\subsection{Notation}

In order to maintain a consistent notation we will be using the following
conventions

\begin{itemize}
\item  $\alpha ,\beta ,\xi ,\phi $ etc. ---Lower case Greek letters for
scalars and scalar valued functions.

\item  $a,b,x,f$ etc. ---Lower case Roman letters for column vectors and
vector valued functions.

\item  $A,B,X,F$ etc. ---Upper case Roman letters for matrices and matrix
valued functions. We also use $\left( a_{ij}\right) $ to denote a matrix
whose $ij^{th}$ element is $a_{ij}$.

\item  $\mathcal{D}_n,\mathcal{\QTR{mathcal}{K}}_{nm},$ etc. ---Upper case
calligraphic letters are for special transformation matrices.

\item  $A^{\prime }$ ---represents the matrix transpose, also $a^{\prime }$
represents a row vector.

\item  $\,\mathrm{tr}\,A$ ---represents the trace of the matrix $A.$

\item  $\left| A\right| $---represents the determinant of the matrix $A.$
\end{itemize}

For example, $\phi \left( X\right) =\left| X\right| $ is a scalar function
of a matrix and $F\left( x\right) =xx^{\prime }$ is a matrix function of a
vector.

\subsection{Kronecker Product}

Let $A$ be an $m\times n$ matrix and $B$ a $p\times q$ matrix. The $mp\times
nq$ matrix defined by 
\begin{equation}
A\otimes B=\left[ 
\begin{array}{ccc}
a_{11}B & \cdots & a_{1n}B \\ 
\vdots & \ddots & \vdots \\ 
a_{m1}B & \cdots & a_{mn}B
\end{array}
\right]
\end{equation}
is called the Kronecker Product of $A$ and $B$. The Kronecker product is a
partitioned matrix whose $ij^{th}$ partition is $a_{ij}$ times $B$.

The properties of the Kronecker product are fairly well known and there are
several good references on the subject, for example, Steeb\cite{Steeb91} or
Graham\cite{Graham81}. Some equalities that are important in the derivations
latter in this paper are, 
\begin{equation}
\left( A+B\right) \otimes \left( C+D\right) =A\otimes C+A\otimes D+B\otimes
C+B\otimes D
\end{equation}
\begin{equation}
\left( A\otimes B\right) \left( C\otimes D\right) =AC\otimes BD
\end{equation}
\begin{equation}
a^{\prime }\otimes b=ba^{\prime }=b\otimes a^{\prime }
\end{equation}
\begin{equation}
\left( A\otimes B\right) ^{\prime }=A^{\prime }\otimes B^{\prime }
\end{equation}
\begin{equation}
\left( A\otimes B\right) ^{-1}=A^{-1}\otimes B^{-1}
\end{equation}
\begin{equation}
\left| A\otimes B\right| =\left| A\right| ^m\left| B\right| ^n\text{ where }A%
\text{ is }n\times n\text{ and }B\text{ is }m\times m  \label{krondet}
\end{equation}
\begin{equation}
\,\mathrm{tr}\,\left( A\otimes B\right) =\left( \,\mathrm{tr}\,A\right)
\left( \,\mathrm{tr}\,B\right)  \label{krontrace}
\end{equation}

\subsection{Hadamard Products and Powers}

If $A=\left( a_{ij}\right) $ and $B=\left( b_{ij}\right) $ are matrices of
the same order, say $m\times n$, then the Hadamard product of $A$ and $B$ is
defined as the $m\times n$ matrix 
\begin{equation}
A\odot B=\left( a_{ij}b_{ij}\right)
\end{equation}
The Hadamard product is simply the corresponding term product of two
matrices.

The properties of the Hadamard product are less widely known. The Hadamard
product is sometimes called the Schur product, for a historical perspective
and a survey of results see Styan\cite{Styan73}, see also the work by Horn
and Johnson\cite{HornJohnson91}.

A property useful in this work is 
\begin{equation}
\,\mathrm{tr}\,\left[ A^{\prime }\left( B\odot C\right) \right] =\,\mathrm{tr%
}\,\left[ \left( A^{\prime }\odot B^{\prime }\right) C\right]  \label{trhad}
\end{equation}
$\,$

In this paper other element-wise actions on a matrix, such as, roots and
inversion, i.e. Hadamard powers are used. The following notation is used: 
\begin{equation}
A^{\left[ \alpha \right] }=\left( a_{ij}^\alpha \right) \text{ when }%
a_{ij}^\alpha \text{ is well defined for all of the }ij\text{ elements in }A.
\end{equation}

\subsection{The vec and vech Operators}

The vec operator transforms a matrix into a vector by stacking the columns
of the matrix one underneath the other.

Let $A$ be an $m\times n$ matrix and $a_j$ its $j^{th}$ column, then $\,%
\mathrm{vec}\,A$ is the $mn\times 1$ vector 
\begin{equation}
\,\mathrm{vec}\,A=\left[ 
\begin{array}{c}
a_1 \\ 
a_2 \\ 
\vdots \\ 
a_n
\end{array}
\right]
\end{equation}
If the original dimensions of $A$ are known then the vec operation is
invertible and the $ij^{th}$ element of $A$ is given by the indexing scheme $%
a_{ij}=\left( \,\mathrm{vec}\,A\right) _{i+\left( j-1\right) m}$.

An operator similar to vec is the vech, ``vector half'', operator. Let $A$
be a square $n\times n$ matrix . Then $\,\mathrm{vech}\,A$ is the $n\left(
n+1\right) /2\times 1$ vector obtained by stacking the lower triangular
elements of $A$. For example, if $n=3$, 
\begin{equation}
\,\mathrm{vech}\,A=\left[ 
\begin{array}{c}
a_{11} \\ 
a_{21} \\ 
a_{31} \\ 
a_{22} \\ 
a_{32} \\ 
a_{33}
\end{array}
\right]
\end{equation}
For symmetric $X$, $\,\mathrm{vech}\,X$ contains the independent elements of 
$X$. For a review of the notation, properties, and some applications of the
vec and vech operators see the papers by Henderson and Searle\cite
{HendersonSearle79,HendersonSearle80}.

Here only a few properties of the vec operator are given. In the section on
linear structures many more relations involving vec and vech will be
presented. 
\begin{equation}
\,\mathrm{vec}\,ab^{\prime }=b\otimes a
\end{equation}
\begin{equation}
\left( \,\mathrm{vec}\,A\right) ^{\prime }\,\mathrm{vec}\,B=\,\mathrm{tr}%
\,\left( A^{\prime }B\right)  \label{tr2vec}
\end{equation}
$\,$%
\begin{equation}
\mathrm{vec}\,\left( ABC\right) =\left( C^{\prime }\otimes A\right) \,%
\mathrm{vec}\,B  \label{vecabc}
\end{equation}

Using (\ref{tr2vec}) and (\ref{trhad}) we find 
\begin{equation}
\left( \,\mathrm{vec}\,\left[ A\odot B\right] \right) ^{\prime }\,\mathrm{vec%
}\,C=\left( \,\mathrm{vec}\,A\right) ^{\prime }\,\mathrm{vec}\,\left[ B\odot
C\right]
\end{equation}
or 
\begin{equation}
\left( \,\mathrm{vec}\,A\odot \,\mathrm{vec}\,B\right) ^{\prime }\,\mathrm{%
vec}\,C=\left( \,\mathrm{vec}\,A\right) ^{\prime }\left( \,\mathrm{vec}%
\,B\odot \,\mathrm{vec}\,C\right)  \label{vechad}
\end{equation}

\subsection{Linear Structures}

When working with matrices as variable arguments a way to account for the
structural properties of the matrices is needed. For example an $n\times n$
symmetric matrix contains only $s=n\left( n+1\right) /2$ independent
elements the other $n\left( n-1\right) /2$ elements being determined by the
linear relations $a_{ij}=a_{ji}$. Magnus\cite{Magnus83,Magnus88} has
developed an elegant and versatile method for dealing with such structural
properties of matrices.

The term, linear structure, as defined by Magnus, is the set of all $m\times
n$ variable matrices that have $mn-s$ linearly independent relations among
their $mn$ elements. This means that a matrix with a given linear structure
contains only $s$ linearly independent terms the rest being determined by
the $mn-s$ relations. A given linear structure defines an $s$ dimensional
subspace of the real vector space $\Bbb{R}^{mn}$. Thus if $A$ is an $m\times
n$ matrix with a given linear structure containing $s$ linearly independent
elements then there exists an $mn\times s$ matrix, $\mathcal{B}$, of basis
vectors for the subspace defined by the linear structure such that 
\begin{equation}
\mathcal{B}\,\mathrm{\,\QTR{mathrm}{vec}}\text{x}\left( A\right) =\,\mathrm{%
vec}\,A\,\,\text{ for some }s\times 1\text{ vector }\mathrm{\QTR{mathrm}{vec}%
}\text{x}\left( A\right)  \label{lindef}
\end{equation}
The vector $\mathrm{\QTR{mathrm}{vec}}$x$\left( A\right) $ contains only
linearly independent elements of $A$. $\mathcal{B}$ is called a basis matrix
for the linear structure. Note that $\mathcal{B}$ and vecx are not unique,
however, once the operator $\mathrm{\QTR{mathrm}{vec}}$x is specified $%
\mathcal{B}$ is uniquely determined. Also note that since $\mathcal{B}$ has
full column rank 
\begin{equation}
\mathcal{B}^{+}\,\mathrm{vec}\,A=\,\mathrm{vec}\text{x}\left( A\right)
\end{equation}
where $\mathcal{B}^{+}$ is the Moore Penrose inverse of $\mathcal{B}$, (see
any of the references\cite{RaoMitra71,BenIsrael74,MagNeud88} for the
definition and properties of the Moore Penrose inverse). The expression (\ref
{lindef}) is taken as a working definition for a linear structure.

The rest of this section gives some general results for the more common
basis matrices. There are two other basis matrices derived later
specifically for use in derivations of the potential energy integral and
derivative.

\subsubsection{The Commutation Matrix}

A special class of linear structures arises when in the definition given
above there are no linear relations. If in (\ref{lindef}) vecx is defined by 
$\,\mathrm{vec}$x$\left( A\right) =\,\mathrm{vec}\,A^{\prime }$ then the
resulting basis matrix (a permutation matrix) is called the commutation
matrix. Other notation for this basis matrix and a review of its history is
given in the paper by Henderson and Searle\cite{HendersonSearle80}.

Let $A$ be an arbitrary $m\times n$ matrix. Then there exists a unique $%
mn\times mn$ permutation matrix $\mathcal{K}_{mn}$ such that 
\begin{equation}
\mathcal{K}_{mn}\,\mathrm{vec}\,A=\,\mathrm{vec}\,A^{\prime }
\end{equation}
The name commutation matrix refers to the action of $\mathcal{K}_{mn}$ on
the Kronecker product of two matrices. Let $A$ be an $m\times n$ matrix and $%
B$ a $p\times q$ matrix, then using (\ref{vecabc}) 
\begin{equation}
\mathcal{K}_{pm}\left( A\otimes B\right) =\left( B\otimes A\right) \mathcal{K%
}_{qn}
\end{equation}
and, noting that $\mathcal{K}_{nm}=\mathcal{K}_{mn}^{\prime }=\mathcal{K}%
_{mn}^{-1}$%
\begin{equation}
\mathcal{K}_{pm}\left( A\otimes B\right) \mathcal{K}_{nq}=B\otimes A
\end{equation}
The commutation matrix also allows, (by an involved proof), the vec of a
Kronecker product to be expressed in terms of a Kronecker product of vecs. 
\begin{equation}
\,\mathrm{vec}\,\left( A\otimes B\right) =\left( I_n\otimes \mathcal{K}%
_{qm}\otimes I_p\right) \left( \,\mathrm{vec}\,A\otimes \,\mathrm{vec}%
\,B\right)
\end{equation}
A possible construction for the matrix $\mathcal{K}_{mn}$ is 
\begin{equation}
\mathcal{K}_{mn}=\sum_{i=1}^m\sum_{j=1}^n\left( E_{ij}\otimes E_{ji}^{\prime
}\right)  \label{conk}
\end{equation}
where $E_{ij}$ is the $m\times n$ matrix with $1$ in its $ij^{th}$ position
and $0$'s elsewhere.

Closely related to the commutation matrix is the $n^2\times n^2$ matrix $%
\mathcal{N}_n$ defined, for any $n\times n$ matrix $A,$ by 
\begin{equation}
\mathcal{N}_n\,\mathrm{vec}\,A=\,\mathrm{vec}\,\frac 12\left( A+A^{\prime
}\right)
\end{equation}
Some properties of $\mathcal{N}_n$ are 
\begin{equation}
\mathcal{N}_n=\frac 12\left( I_{n^2}+\mathcal{K}_{nn}\right)
\end{equation}
\begin{equation}
\mathcal{N}_n\mathcal{K}_{nn}=\mathcal{N}_n=\mathcal{K}_{nn}\mathcal{N}_n
\end{equation}
\begin{equation}
\mathcal{N}_n\left( A\otimes A\right) \mathcal{N}_n=\mathcal{N}_n\left(
A\otimes A\right) =\left( A\otimes A\right) \mathcal{N}_n
\end{equation}

\subsubsection{The Duplication Matrix and vech for Symmetric Matrices}

Symmetry about the principle diagonal is a very important linear structure;
for this case the basis matrix is called the duplication matrix. The
duplication matrix $\mathcal{D}_n$ is an $n^2\times n(n+1)/2$ matrix such
that for an $n\times n$ symmetric matrix $A$%
\begin{equation}
\mathcal{D}_n\,\mathrm{vech}\,A=\,\mathrm{vec}\,A  \label{Dvech}
\end{equation}
\begin{equation}
\mathcal{D}_n^{+}\,\mathrm{vec}\,A=\,\mathrm{vech}\,A
\end{equation}
and for an arbitrary $n\times n$ matrix $A$%
\begin{equation}
\mathcal{D}_n^{\prime }\,\mathrm{vec}\,A=\,\mathrm{vech}\,\left( A+A^{\prime
}-\,\mathrm{diag}\,A\right)
\end{equation}
Justification for the name, duplication matrix, is apparent in the action of 
$\mathcal{D}_n$ on $\,\mathrm{vech}\,A$, equation (\ref{Dvech}).

Some useful relationships between $\mathcal{D}_n$, $\mathcal{K}_{nn}$ and $%
\mathcal{N}_n$ are 
\begin{equation}
\mathcal{K}_{nn}\mathcal{D}_n=\mathcal{D}_n=\mathcal{N}_n\mathcal{D}_n
\end{equation}
\begin{equation}
\mathcal{D}_n^{+}\mathcal{K}_{nn}=\mathcal{D}_n^{+}=\mathcal{D}_n^{+}%
\mathcal{N}_n
\end{equation}
\begin{equation}
\mathcal{D}_n\mathcal{D}_n^{+}=\mathcal{N}_n
\end{equation}

A possible construction for $\mathcal{D}_n$ is given by 
\[
\mathcal{D}_n=\sum_{i\geq j}\left( \,\mathrm{vec}\,T_{ij}\right)
u_{ij}^{\prime } 
\]
where 
\begin{equation}
T_{ij}=\left\{ 
\begin{array}{ll}
E_{ij}+E_{ij} & \text{if }i\neq j \\ 
E_{ij} & \text{if }i=j
\end{array}
\right.  \label{tijdef}
\end{equation}
with $E_{ij}$ $\left( n\times n\right) $ defined as in (\ref{conk}) and $%
u_{ij}$ $\left( i\geq j\right) $ is a $n(n+1)/2\times 1$ unit vector with $1$
in it's $\left[ \left( j-1\right) n+i-j\left( j-1\right) /2\right] $
position and zeros elsewhere.

\subsubsection{The Elimination Matrix and Lower Triangular Matrices}

Another important linear structure is lower triangularity. The basis matrix
of this linear structure is called the elimination matrix. Note that if $L$
is a lower triangular matrix the independent elements of $L$ are contained
in the vector $\,\mathrm{vech}\,L$.

Let $A$ be an $n\times n$ lower triangular matrix. The elimination matrix, $%
\mathcal{L}_n$, is the $n\left( n+1\right) /2\times n^2$ matrix defined by 
\begin{equation}
\mathcal{L}_n^{\prime }\,\mathrm{vech}\,A=\,\mathrm{vec}\,A  \label{ldef}
\end{equation}
Note that the transpose of $\mathcal{L}_n$ was used in (\ref{ldef}). The
name, elimination matrix, is from the action of $\mathcal{L}_n$ on the vec
of a lower triangular matrix 
\begin{equation}
\mathcal{L}_n\,\mathrm{vec}\,L=\,\mathrm{vech}\,L
\end{equation}
$\mathcal{L}_n$ eliminates the zero elements from $\,\mathrm{vec}\,L$ when $%
L $ is lower triangular, whence the name.

Some of the relationships between $\mathcal{L}_n$, $\mathcal{D}_n$, and $%
\mathcal{N}_n$ are 
\begin{equation}
\mathcal{L}_n\mathcal{D}_n=I_{\frac 12n\left( n+1\right) }
\end{equation}
\begin{equation}
\mathcal{D}_n\mathcal{L}_n\mathcal{N}_n=\mathcal{N}_n
\end{equation}
\begin{equation}
\mathcal{D}_n^{+}=\mathcal{L}_n\mathcal{N}_n
\end{equation}

An explicit expression for $\mathcal{L}_n$ is 
\begin{equation}
\mathcal{L}_n=\sum_{i\geq j}u_{ij}\left( \,\mathrm{vec}\,E_{ij}\right)
^{\prime }
\end{equation}
where $u_{ij}$ and $E_{ij}$ are defined following equation (\ref{tijdef}).

\subsection{Matrix Differential Calculus}

It is not the intention of this paper to present a rigorous and thorough
description of the matrix differential calculus. The goal is merely to
present enough information so that the reader will be able to follow the
derivations that come later. A thorough, mathematically rigorous,
development and very practical guide to the use of matrix differential
calculus can be found in the book by Magnus and Neudecker\cite{MagNeud88}.

\subsubsection{Notation for Matrix Derivatives}

Given an $m\times n$ matrix function $F\left( X\right) $ of a $p\times q$
matrix variable $X$, there are $mnpq$ partial derivatives which are to be
arranged in a practical matrix derivative of $F$ with respect to $X$. There
are three methods of displaying these partial derivatives in common use. The
first two methods display the derivatives in Kronecker product orders and
although these orders may seem natural they lead to problems. The first
method, used by Graham\cite{Graham81}, is an $mp\times nq$ partitioned
matrix of the form 
\begin{equation}
\left( \frac{\partial F\left( X\right) }{\partial x_{ij}}\right)
\label{def1}
\end{equation}
whose $ij^{th}$ partition is obtained by replacing the $kl^{th}$ element of $%
F$ with the partial derivative $\partial F_{kl}/\partial x_{ij}$ for all $mn$
elements in $F$. This form has the same partitioning as the Kronecker
product $X\otimes F$. The second method for arranging the partial
derivatives, used by Rogers\cite{Rogers80} and Balestra\cite{Balestra75}, is
an $mp\times nq$ partitioned matrix of the form 
\begin{equation}
\left( \frac{\partial F\left( X\right) _{kl}}{\partial X}\right)
\label{def2}
\end{equation}
whose $kl^{th}$ partition is obtained by replacing the $ij^{th}$ element of $%
X$ with the partial derivative $\partial F_{kl}/\partial x_{ij}$ for all $pq$
elements in $X$. This form has the same partitioning as the Kronecker
product $F\otimes X$. Both of these forms have serious theoretical and
practical problems. For a discussion of the theoretical considerations see
the paper by Pollock\cite{Pollock85}. One of the practical problems can be
seen with an easy example. Consider the identity transformation $F\left(
X\right) =X$ where $X$ is $n\times n$. Both (\ref{def1}) and (\ref{def2})
give the derivative of $F$ with respect to $X$ as $\sum_{i,j}^n\left(
E_{ij}\otimes E_{ij}\right) $ which is a matrix of rank one. The Jacobian
matrix of the identity transformation on $n^2$ variables is obviously the $%
n^2\times n^2$ identity matrix which has rank $n^2$. Thus the matrix of
partial derivatives defined with either (\ref{def1}) or (\ref{def2}) is not
a Jacobian matrix since the rank of the transformation is not preserved.
Also neither of these definitions lead to a useful chain rule, see Pollock%
\cite{Pollock85}. These forms do not conform with our standard ideas from
analysis and linear algebra.

A practical and consistent definition of the matrix derivative should be a
logical extension of the notion of a Jacobian matrix of a vector function
and facilitate a natural chain rule. The following definition satisfies
these criteria and others, (see Magnus and Neudecker\cite{MagNeud88}, and
Pollock\cite{Pollock85}).

For $F\left( X\right) $ an $m\times n$ matrix function of the $p\times q$
matrix variable $X$, the derivative or Jacobian of $F$ with respect to $X\,$
is the $mn\times pq$ matrix of partial derivatives 
\begin{equation}
\frac{\partial \,\mathrm{vec}\,F\left( X\right) }{\partial \left( \,\mathrm{%
vec}\,X\right) ^{\prime }}  \label{derdef}
\end{equation}
whose $ij^{th}$ element is the partial derivative of the $i^{th}$ component
of $\,\mathrm{vec}\,F\left( X\right) $, a column vector, with respect to the 
$j^{th}$ element of $\left( \,\mathrm{vec}\,X\right) ^{\prime },$ a row
vector. This form of the matrix derivative is very flexible and easy to work
with since only vector forms of the matrices are used. Whence, all of the
information about linear structures and standard matrix algebra can be used
when working with matrix derivatives in this form. This is the form used in
Magnus and Neudecker\cite{MagNeud88} and in this work.

Note that when using definition (\ref{derdef}) for the derivative the
gradient of a matrix function with respect to a matrix is defined as the
transpose of the derivative 
\begin{equation}
\nabla _X\,F\left( X\right) =\left( \frac{\partial \,\mathrm{vec}\,F\left(
X\right) }{\partial \left( \,\mathrm{vec}\,X\right) ^{\prime }}\right)
^{\prime }
\end{equation}
Thus, for a scalar valued function such as $\phi \left( X\right) $ the
derivative is a row vector while the gradient is a column vector in keeping
with linear algebraic conventions.

To reiterate, if $F\left( X\right) $ is an $m\times n$ matrix function of a $%
p\times q$ matrix variable $X$ then, the derivative of $F$ with respect to $%
X $ is $\partial \,\mathrm{vec}\,F\left( X\right) /\partial \left( \,\mathrm{%
vec}\,X\right) ^{\prime }$ which is an $mn\times pq$ matrix, whose $ij^{th}$
element is the partial derivative of the $i^{th}$ component of $\,\mathrm{vec%
}\,F\left( X\right) $ with respect to the $j^{th}$ element of $\,\mathrm{vec}%
\,X$. At this point the reader may have questions concerning formal
definitions of differentiability, existence and uniqueness proofs, etc.; for
all of these concerns the reader is referred to Magnus and Neudecker\cite
{MagNeud88,MagNeud85}.

\subsubsection{Differentials and Derivatives}

Matrix derivatives are found using differentials as follows: If a matrix $A$
can be found such that 
\begin{equation}
\,\mathrm{vec}\,dF\left( X\right) =A\,d\,\mathrm{vec}\,X
\end{equation}
then the derivative is identified as the matrix $A$%
\begin{equation}
A=\frac{\partial \,\mathrm{vec}\,F\left( X\right) }{\partial \left( \,%
\mathrm{vec}\,X\right) ^{\prime }}
\end{equation}
Using our notational conventions, this method of identifying matrix
derivatives from differentials is summarized in Table~\ref{table1}\cite
{MagNeud88}.

\TeXButton{B}{\begin{table}[h] \centering} 
\begin{tabular}[t]{llll}
\hline\hline
Function & Differential & Derivative/Jacobian & Size \\ \hline
$\phi \left( \xi \right) $ & $d\,\phi =\alpha \,d\xi $ & $\frac{\partial
\phi \left( \xi \right) }{\partial \xi }=\alpha $ & $1\times 1$ \\ 
$\phi \left( x\right) $ & $d\phi =a^{\prime }\,dx$ & $\frac{\partial \phi
\left( x\right) }{\partial x^{\prime }}=a^{\prime }$ & $1\times s$ \\ 
$\phi \left( X\right) $ & $d\phi =\,\mathrm{tr}\,\left( A^{\prime
}\,dX\right) $ & $\frac{\partial \phi \left( X\right) }{\partial \left( \,%
\mathrm{vec}\,X\right) ^{\prime }}=\left( \,\mathrm{vec}\,A\right) ^{\prime
} $ & $1\times pq$ \\ 
& $\;\;=\left( \,\mathrm{vec}\,A\right) ^{\prime }d\,\mathrm{vec}\,X$ &  & 
\\ 
$f\left( \xi \right) $ & $df=a\,d\xi $ & $\frac{\partial f\left( \xi \right) 
}{\partial \xi }=a$ & $r\times 1$ \\ 
$f\left( x\right) $ & $df=A\,dx$ & $\frac{\partial f\left( x\right) }{%
\partial x^{\prime }}=A$ & $r\times s$ \\ 
$f\left( X\right) $ & $df=A\,d\,\mathrm{vec}\,X$ & $\frac{\partial f\left(
X\right) }{\partial \left( \,\mathrm{vec}\,X\right) ^{\prime }}=A$ & $%
r\times pq$ \\ 
$F\left( \xi \right) $ & $dF=A\,d\xi $ & $\frac{\partial \,\mathrm{vec}%
\,F\left( \xi \right) }{\partial \xi }=\,\mathrm{vec}\,A$ & $mn\times 1$ \\ 
$F\left( x\right) $ & $d\,\mathrm{vec}\,F=A\,dx$ & $\frac{\partial \,\mathrm{%
vec}\,F\left( x\right) }{\partial x^{\prime }}=\,A$ & $mn\times s$ \\ 
$F\left( X\right) $ & $d\,\mathrm{vec}\,F=A\,d\,\mathrm{vec}\,X$ & $\frac{%
\partial \,\mathrm{vec}\,F\left( X\right) }{\partial \left( \,\mathrm{vec}%
\,X\right) ^{\prime }}=\,A$ & $mn\times pq$ \\ \hline\hline
\end{tabular}
\caption{The First Derivative Identification Table: $F$ is an  $m\times n$ matrix function,  $f$ is an $r\times 1$ vector function, $x$ is a
$s\times 1$ vector variable, $X$ is a  $p\times q$ matrix variable\label{table1}}%
\TeXButton{E}{\end{table}}

Table~\ref{table1} is used to identify the matrix derivative once the
differential has been found and arranged in a form given in the table.
Finding the differential is usually easy. Some fundamental rules for
differential of matrix functions are given below.

Let $U$ and $V$ be matrix functions and $A$ a matrix of real constants, then 
\begin{equation}
dA=0
\end{equation}
\begin{equation}
d\left( \alpha U\right) =\alpha dU
\end{equation}
\begin{equation}
d\left( U\pm V\right) =\,dU\pm \,dV
\end{equation}
\begin{equation}
d\left( UV\right) =\,\left( dU\right) V+U\,dV
\end{equation}
\begin{equation}
d\left( U\otimes V\right) =\left( dU\right) \otimes V+U\otimes \,dV
\end{equation}
\begin{equation}
d\left( U\odot V\right) =\left( dU\right) \odot V+U\odot \,dV
\end{equation}
\begin{equation}
dU^{\prime }=\left( dU\right) ^{\prime }
\end{equation}
\begin{equation}
d\,\mathrm{vec}\,U=\,\mathrm{vec}\,dU
\end{equation}
\begin{equation}
d\,\mathrm{tr}\,U=\,\mathrm{tr}\,dU  \label{dtrace}
\end{equation}

In our later calculations derivatives for the matrix functions trace,
determinant, inverse, and eigenvalue will be needed. These derivatives may
not be immediately obvious so they are derived here for later reference.

Let $\phi \left( X\right) =\,\mathrm{tr}\,\left[ AX\right] $ then, using (%
\ref{tr2vec}) 
\begin{equation}
d\left( \,\mathrm{tr}\,\left[ AX\right] \right) =d\left( \left( \,\mathrm{vec%
}\,A^{\prime }\right) ^{\prime }\,\mathrm{vec}\,X\right) =\left( \,\mathrm{%
vec}\,A^{\prime }\right) ^{\prime }d\,\mathrm{vec}\,X
\end{equation}
thus, 
\begin{equation}
\frac{\partial \,\mathrm{tr}\,\left[ AX\right] }{\partial \left( \,\mathrm{%
vec}\,X\right) ^{\prime }}=\left( \,\mathrm{vec}\,A^{\prime }\right)
^{\prime }
\end{equation}
This result should be clear from Table~\ref{table1} and equation (\ref
{dtrace}).

Let $X$ be a full rank $n\times n$ matrix with elements $x_{ij},$ and let $%
c_{ij}$ be the corresponding cofactors, then the determinant of $X$ may be
written 
\begin{equation}
\left| X\right| =\sum_{i=1}^nc_{ij}x_{ij}
\end{equation}
thus, 
\begin{equation}
\frac{\partial \left| X\right| }{\partial x_{ij}}=c_{ij}
\end{equation}
so the differential is 
\begin{equation}
d\left| X\right| =\sum_{i=1}^n\sum_{j=1}^nc_{ij}\,\,d\,x_{ij}=\,\mathrm{tr}%
\,X^{\#}\,d\,X
\end{equation}
where $X^{\#}$ is the adjoint of $X$. Using $X^{\#}=\left| X\right| X^{-1}$
gives 
\begin{eqnarray}
d\left| X\right| &=&\left| X\right| \,\mathrm{tr}\,\left[ X^{-1}\,d\,X\right]
\nonumber \\
&=&\left| X\right| \left( \,\mathrm{vec}\,\left( X^{-1}\right) ^{\prime
}\right) ^{\prime }d\,\,\mathrm{vec}\,X
\end{eqnarray}
Therefore, 
\begin{equation}
\frac{\partial \left| X\right| }{\partial \left( \,\mathrm{vec}\,X\right)
^{\prime }}=\left| X\right| \left( \,\mathrm{vec}\,\left( X^{-1}\right)
^{\prime }\right) ^{\prime }
\end{equation}

For the inverse of a non-singular $n\times n$ matrix $X$, 
\begin{equation}
0=d\left( I_n\right) =d\left( XX^{-1}\right) =\left( d\,X\right)
X^{-1}+X\,\,d\,X^{-1}
\end{equation}
Multiplying on the right by $X^{-1}$we find 
\begin{equation}
d\,\,X^{-1}=-X^{-1}\left( d\,X\right) X^{-1}
\end{equation}
applying the vec operator and using identity (\ref{vecabc}) gives 
\begin{equation}
d\,\,\mathrm{vec}\,X^{-1}=-\left( \left( X^{\prime }\right) ^{-1}\otimes
X^{-1}\right) d\,\,\mathrm{vec}\,X
\end{equation}
Thus, the derivative is 
\begin{equation}
\frac{\partial \,\mathrm{vec}\,X^{-1}}{\partial \left( \,\mathrm{vec}%
\,X\right) ^{\prime }}=-\left( \left( X^{\prime }\right) ^{-1}\otimes
X^{-1}\right)
\end{equation}

The symmetric eigenvalue equation $Xu=\lambda u$ implicitly defines a
differentiable scalar function $\lambda \left( X\right) $ and a vector
function $u\left( X\right) $ (see Magnus and Neudecker\cite{MagNeud88}).
Taking the differential of the eigenvalue equation gives 
\begin{equation}
\left( d\,X\right) u+X\,\,d\,u=\left( d\,\lambda \right) u+\lambda \,\,d\,u
\end{equation}
Multiplying by $u^{\prime }$ from the left leads to 
\begin{equation}
u^{\prime }\left( d\,X\right) u+u^{\prime }X\,\,d\,u=u^{\prime }\left(
d\,\lambda \right) u+\lambda u^{\prime }\,d\,u
\end{equation}
Now, noting that $u^{\prime }X=\lambda u^{\prime }$ since $X$ is symmetric,
we have after simplification, applying vec, and using (\ref{vecabc}) 
\begin{eqnarray}
d\,\lambda &=&\frac 1{u^{\prime }u}u^{\prime }\left( d\,X\right) u  \nonumber
\\
\ &=&\frac 1{u^{\prime }u}\left( u^{\prime }\otimes u^{\prime }\right)
\,d\,\,\mathrm{vec}\,X
\end{eqnarray}
Hence, the derivative is 
\begin{equation}
\frac{\partial \lambda }{\partial \left( \,\mathrm{vec}\,X\right) ^{\prime }}%
=\frac 1{u^{\prime }u}\left( u^{\prime }\otimes u^{\prime }\right)
\end{equation}
For completeness, (it will not be needed in this paper), the result for the
eigenvector function is, 
\begin{eqnarray}
d\,u &=&\left( \lambda I_n-X\right) ^{+}\left( d\,X\right) u  \nonumber \\
\ &=&\left( u^{\prime }\otimes \left( \lambda I_n-X\right) ^{+}\right) d\,\,%
\mathrm{vec}\,X
\end{eqnarray}
and 
\begin{equation}
\frac{\partial u}{\partial \left( \,\mathrm{vec}\,X\right) ^{\prime }}%
=u^{\prime }\otimes \left( \lambda I_n-X\right) ^{+}
\end{equation}
where $\left( \lambda I_n-X\right) ^{+}$ is a Moore Penrose inverse (for the
derivation see Magnus and Neudecker\cite{MagNeud88}).

\section{Correlated Gaussian Basis Functions}

The basic form for an $n$ particle correlated Gaussian function used in this
paper is the negative exponential of a positive definite quadratic form in $%
3n$ variables 
\begin{equation}
\phi _k=\exp \left[ -r^{\prime }\left( L_kL_k^{\prime }\otimes I_3\right)
r\right]
\end{equation}
here $r$ is a $3n\times 1$ vector of Cartesian coordinates for the $n$
particles, $L_k$ is an $n\times n$ lower triangular matrix of rank $n$ and $%
I_3$ is the $3\times 3$ identity matrix. $k$ would range from $1$ to $N$
where $N$ is the number of basis functions. The Kronecker product with $I_3$
is used to insure rotational invariance of the basis functions. Also,
integrals involving the functions $\phi _k$ are well defined only if the
exponent matrix is positive definite symmetric, this is assured by using the
Cholesky factorization $L_kL_k^{\prime }$. The following simplifications
will help keep the notation more compact 
\begin{eqnarray}
L_kL_k^{\prime } &=&A_k \\
A_k+A_l &=&A_{kl} \\
\bar A &=&A\otimes I_3
\end{eqnarray}
With the above notational simplifications the most compact form for $\phi _k$
is 
\begin{equation}
\phi _k=\exp \left[ -r^{\prime }\bar A_kr\right]
\end{equation}
The function $\phi _k$ can be thought of as both a scalar valued vector
function of $r$ and a scalar valued matrix function of $L_k$. The
Hamiltonian and overlap matrixes constructed from a basis of the $\phi _k$'s
are then matrix functions of the matrices $L_k$, and the energy functional
is a differentiable function with respect to the independent variational
parameters contained in the column vector 
\begin{equation}
a=\left[ \left( \mathrm{vech}\,L_1\right) ^{\prime },\cdots ,\left( \mathrm{%
vech}\,L_N\right) ^{\prime }\right] ^{\prime }  \label{avec}
\end{equation}

\section{Integrals For Correlated Gaussians}

The following well known\cite{Friedman90:int} integral is the foundation for
the derivations that follow.

\begin{equation}
\int_{-\infty }^\infty e^{-x^{\prime }Ax}dx=\pi ^{n/2}\left| A\right| ^{-1/2}
\label{baseint}
\end{equation}
Here $A$ is a positive definite $n\times n$ matrix and $x$ is a $n\times 1$
vector variable.

\subsection{Overlap}

The overlap integral follows simply from the result in equation (\ref
{baseint}). Noting that $\left( A_k\otimes I_3\right) +\left( A_l\otimes
I_3\right) =\left( A_k+A_l\right) \otimes I_3=A_{kl}\otimes I_3=\bar A_{kl}$%
. We have, 
\begin{eqnarray}
\left\langle \phi _k\right. |\left. \phi _l\right\rangle &=&\pi
^{3n/2}\left| \bar A_{kl}\right| ^{-1/2}  \nonumber \\
\ &=&\pi ^{3n/2}\left| A_{kl}\otimes I_3\right| ^{-1/2}  \nonumber \\
\ &=&\pi ^{3n/2}\left( \left| A_{kl}\right| ^3\left| I_3\right| ^n\right)
^{-1/2}\text{ (from eqn (\ref{krondet}))}  \nonumber \\
\ &=&\pi ^{3n/2}\left| A_{kl}\right| ^{-3/2}  \label{overlap1}
\end{eqnarray}
When $k=l$, (\ref{overlap1}) simplifies further 
\begin{eqnarray}
\left\langle \phi _k\right. |\left. \phi _k\right\rangle &=&\pi
^{3n/2}\left| 2L_kL_k^{\prime }\right| ^{-3/2}  \nonumber \\
&=&\left( \frac \pi 2\right) ^{3n/2}\left| L_k\right| ^{-3}
\end{eqnarray}
In numerical calculations it is advisable to use normalized basis functions
in order to avoid problems with ill-conditioned overlap and Hamiltonian
matrices. Therefore, overlap matrix elements, $S_{kl}$, are defined using
normalized basis functions. After simplification 
\begin{equation}
S_{kl}=\frac{\left\langle \phi _k\right. |\left. \phi _l\right\rangle }{%
\left( \left\langle \phi _k\right. |\left. \phi _k\right\rangle \left\langle
\phi _l\right. |\left. \phi _l\right\rangle \right) ^{1/2}}=2^{3n/2}\left( 
\frac{\left| L_k\right| \left| L_l\right| }{\left| A_{kl}\right| }\right)
^{3/2}  \label{skl}
\end{equation}
Note that this normalized form of the overlap requires very little
additional work since the determinant of $L_k$ is simply the product of it's
diagonal elements.

\subsection{Kinetic Energy}

In center of mass coordinates the kinetic energy operator including mass
polarization can be written in the familiar form\cite{Kinghorn93} 
\begin{equation}
\hat T=-\frac 12\left( \sum_i^n\frac 1{\mu _i}\nabla _i^2+\sum_{i,j}^n\frac
1m\nabla _i\cdot \nabla _j\right)  \label{top1}
\end{equation}
where the $\mu _i$ are reduced masses, $m$ is a real particle mass (usually
chosen as the mass of a heavy particle), and $\nabla _i$ is a gradient with
respect to the $x,y,z$ coordinates of particle $i$. The kinetic energy
operator above, including the mass polarization terms, can be written as 
\begin{equation}
\hat T=-\nabla _r\cdot \left( M\otimes I_3\right) \nabla _r  \label{top2}
\end{equation}
where $\nabla _r$ is the gradient operator with respect to $r,$ the vector
of all particle coordinates, and $M$ is a constant matrix containing the
mass coefficients, $1/2\mu _i$ on the diagonal, for the kinetic terms, and $%
1/2m$ for the off diagonal, mass polarization terms. This is actually a very
general form for a kinetic energy operator since $M$ need not be restricted
to the definition given above. Note that here and throughout the rest of the
paper atomic units are used, hence, $\hbar =1$.

To evaluate the integral $\left\langle \phi _k\right| -\nabla _r\cdot \left(
M\otimes I_3\right) \nabla _r\left| \phi _l\right\rangle $ a few preliminary
results will be needed the first of which is the gradient of $\phi _k$ with
respect to the coordinate vector $r$. Taking the first differential we find, 
\begin{eqnarray}
d\phi _k &=&-\phi _k\,d\left[ r^{\prime }\bar A_kr\right]  \nonumber \\
\ &=&-2\phi _kr^{\prime }\bar A_k\,dr
\end{eqnarray}
Thus, 
\begin{equation}
\frac{\partial \phi _k}{\partial r^{\prime }}=-2\phi _kr^{\prime }\bar
A_k\;\;\;\text{ and }\;\;\;\nabla _r\phi _k=-2\phi _k\bar A_kr
\label{gradfr}
\end{equation}
Next, two forms of the derivative of the overlap integral with respect to $\,%
\mathrm{vec}\,\bar A_{kl}$ will be found one with the derivative taken
inside the integral and the other taken after the integral evaluation, i.e., 
\begin{equation}
\int_{-\infty }^\infty \frac \partial {\partial \left( \,\mathrm{vec}\,\bar
A_{kl}\right) ^{\prime }}\phi _k\phi _ldr\;\;\;\text{and}\;\;\;\frac
\partial {\partial \left( \,\mathrm{vec}\,\bar A_{kl}\right) ^{\prime
}}\int_{-\infty }^\infty \phi _k\phi _ldr
\end{equation}
First, 
\begin{eqnarray}
d\left[ \phi _k\phi _l\right] &=&-\phi _k\phi _l\,d\left[ r^{\prime }\bar
A_{kl}r\right]  \nonumber \\
\ &=&-\phi _k\phi _l\left[ r^{\prime }\,d\bar A_{kl}r\right]  \nonumber \\
\ &=&-\phi _k\phi _l\,\mathrm{tr}\,\left[ rr^{\prime }\,d\bar A_{kl}\right]
\end{eqnarray}
so that, 
\begin{equation}
\frac \partial {\partial \left( \,\mathrm{vec}\,\bar A_{kl}\right) ^{\prime
}}\phi _k\phi _l=-\phi _k\phi _l\left( \,\mathrm{vec}\,rr^{\prime }\right)
^{\prime }
\end{equation}
whence, 
\begin{equation}
\frac \partial {\partial \left( \,\mathrm{vec}\,\bar A_{kl}\right) ^{\prime
}}\left\langle \phi _k\right. |\left. \phi _l\right\rangle =-\left\langle
\phi _k\right| \left( \,\mathrm{vec}\,rr^{\prime }\right) ^{\prime }\left|
\phi _l\right\rangle  \label{overwrtA1}
\end{equation}
Also, using (\ref{overlap1}) we have, 
\begin{equation}
\frac \partial {\partial \left( \,\mathrm{vec}\,\bar A_{kl}\right) ^{\prime
}}\left\langle \phi _k\right. |\left. \phi _l\right\rangle =\frac \partial
{\partial \left( \,\mathrm{vec}\,\bar A_{kl}\right) ^{\prime }}\left( \pi
^{3n/2}\left| \bar A_{kl}\right| ^{-1/2}\right)
\end{equation}
then taking the differential, 
\begin{eqnarray}
d\left\langle \phi _k\right. |\left. \phi _l\right\rangle &=&-\frac 12\pi
^{3n/2}\left| \bar A_{kl}\right| ^{-3/2}d\left| \bar A_{kl}\right|  \nonumber
\\
\ &=&-\frac 12\pi ^{3n/2}\left| \bar A_{kl}\right| ^{-1/2}\,\mathrm{tr}%
\,\left[ \bar A_{kl}^{-1}\,d\bar A_{kl}\right]
\end{eqnarray}
gives 
\begin{equation}
\frac \partial {\partial \left( \,\mathrm{vec}\,\bar A_{kl}\right) ^{\prime
}}\left\langle \phi _k\right. |\left. \phi _l\right\rangle =-\frac
12\left\langle \phi _k\right. |\left. \phi _l\right\rangle \left( \,\mathrm{%
vec}\,\bar A_{kl}^{-1}\right) ^{\prime }  \label{overwrtA2}
\end{equation}
Thus, equating (\ref{overwrtA1}) and (\ref{overwrtA2}) establishes the
equality 
\begin{equation}
\left\langle \phi _k\right| \left( \,\mathrm{vec}\,rr^{\prime }\right)
^{\prime }\left| \phi _l\right\rangle =\frac 12\left\langle \phi _k\right.
|\left. \phi _l\right\rangle \left( \,\mathrm{vec}\,\bar A_{kl}^{-1}\right)
^{\prime }
\end{equation}
The kinetic energy integral can now be evaluated in matrix form 
\begin{eqnarray}
\left\langle \phi _k\right| -\nabla _r\cdot \bar M\nabla _r\left| \phi
_l\right\rangle &=&\left\langle \nabla _r^{\prime }\phi _k\right| \bar
M\left| \nabla _r\phi _l\right\rangle  \nonumber \\
\ &=&4\left\langle \phi _k\right| r^{\prime }\bar A_k\bar M\bar A_lr\left|
\phi _l\right\rangle  \nonumber \\
\ &=&4\left\langle \phi _k\right| \,\mathrm{tr}\,\left[ rr^{\prime }\bar
A_k\bar M\bar A_l\right] \left| \phi _l\right\rangle  \nonumber \\
\ &=&4\left\langle \phi _k\right| \left( \,\mathrm{vec}\,rr^{\prime }\right)
^{\prime }\,\mathrm{vec}\left[ \,\bar A_k\bar M\bar A_l\right] \left| \phi
_l\right\rangle  \nonumber \\
\ &=&4\left\langle \phi _k\right| \left( \,\mathrm{vec}\,rr^{\prime }\right)
^{\prime }\left| \phi _l\right\rangle \,\mathrm{vec}\left[ \,\bar A_k\bar
M\bar A_l\right]  \nonumber \\
\ &=&2\left\langle \phi _k\right. |\left. \phi _l\right\rangle \left( \,%
\mathrm{vec}\,\bar A_{kl}^{-1}\right) ^{\prime }\,\mathrm{vec}\left[ \,\bar
A_k\bar M\bar A_l\right]  \nonumber \\
\ &=&2\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,\mathrm{tr}%
\,\left[ \bar A_{kl}^{-1}\bar A_k\bar M\bar A_l\right]  \nonumber \\
\ &=&2\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,\mathrm{tr}%
\,\left[ \left( A_{kl}^{-1}\otimes I_3\right) \left( A_k\otimes I_3\right)
\left( M\otimes I_3\right) \left( A_l\otimes I_3\right) \right]  \nonumber \\
\ &=&2\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,\mathrm{tr}%
\,\left[ A_{kl}^{-1}A_kMA_l\otimes I_3\right]  \nonumber \\
\ &=&2\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,\mathrm{tr}%
\,\left[ MA_kA_{kl}^{-1}A_l\right] \,\mathrm{tr}\,\left[ I_3\right] 
\nonumber \\
\ &=&6\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,\mathrm{tr}%
\,\left[ MA_kA_{kl}^{-1}A_l\right]
\end{eqnarray}
Thus, using normalized basis functions the kinetic energy matrix elements, $%
T_{kl}$ are given by 
\begin{equation}
T_{kl}=6S_{kl}\,\mathrm{tr}\,\left[ MA_kA_{kl}^{-1}A_l\right]  \label{Tkl}
\end{equation}

\subsection{Potential Energy}

Evaluation of the potential energy integral begins with a reasonably
straight forward single term integral, then the potential energy operator is
written in matrix form using the coordinate vectors and two newly derived
basis matrices. With the operator in matrix form a strikingly similar form
for the matrix elements of this operator is then displayed.

The integral 
\begin{equation}
\left\langle \phi _k\right| \frac 1{r_{ij}}\left| \phi _l\right\rangle
\label{rijint}
\end{equation}
can be evaluated with the help of the identity 
\begin{equation}
\frac 1{r_{ij}}=\frac 2{\sqrt{\pi }}\int_0^\infty e^{-u^2r_{ij}^2}\,du
\label{intur}
\end{equation}
The square of the interparticle distance can be written as 
\begin{equation}
r_{ij}^2=r_i\cdot r_i+r_j\cdot r_j-r_i\cdot r_j-r_j\cdot r_i\;\;\;\text{with}%
\;\;\;r_{ii}^2=r_i^2=r_i\cdot r_i  \label{rij2}
\end{equation}
where $r_i$ is the coordinate vector of the $i^{th}$ particle. If $J_{ij}$
is defined as the matrix, 
\begin{equation}
J_{ij}=\left\{ 
\begin{array}{ll}
E_{ii} & i=j \\ 
E_{ii}+E_{jj}-E_{ij}-E_{ji} & i\neq j
\end{array}
\right.  \label{Jij}
\end{equation}
with $E_{ij}$ as defined in equation (\ref{conk}), $r_{ij}^2$ can also be
written as 
\begin{equation}
r_{ij}^2=r^{\prime }\left( J_{ij}\otimes I_3\right) r  \label{rijsqr}
\end{equation}
Using (\ref{rijsqr}) and (\ref{intur}) the integral (\ref{rijint}) can be
evaluated as follows. 
\begin{eqnarray}
\left\langle \phi _k\right| \frac 1{r_{ij}}\left| \phi _l\right\rangle
&=&\frac 2{\sqrt{\pi }}\left\langle \phi _k\right| \int_0^\infty
e^{-u^2r^{\prime }\left( J_{ij}\otimes I_3\right) r}\,du\left| \phi
_l\right\rangle  \nonumber \\
\ &=&\frac 2{\sqrt{\pi }}\int_0^\infty \int_{-\infty }^\infty \exp \left[
-r^{\prime }\left( \left( A_{kl}\otimes I_3\right) +\left( u^2J_{ij}\otimes
I_3\right) \right) r\right] \,dr\,du  \nonumber \\
\ &=&\frac 2{\sqrt{\pi }}\int_0^\infty \pi ^{3n/2}\left| \left(
A_{kl}+u^2J_{ij}\right) \otimes I_3\right| ^{-1/2}\,du  \nonumber \\
\ &=&\frac 2{\sqrt{\pi }}\int_0^\infty \pi ^{3n/2}\left|
A_{kl}+u^2J_{ij}\right| ^{-3/2}\,du  \nonumber \\
\ &=&\frac 2{\sqrt{\pi }}\int_0^\infty \pi ^{3n/2}\left| A_{kl}\right|
^{-3/2}\left| I_n+u^2J_{ij}A_{kl}\right| ^{-3/2}\,du  \nonumber \\
\ &=&\frac 2{\sqrt{\pi }}\left\langle \phi _k\right. |\left. \phi
_l\right\rangle \int_0^\infty \left| I_n+u^2J_{ij}A_{kl}\right| ^{-3/2}\,du
\end{eqnarray}
The determinant in this last integral is 
\begin{equation}
\left| I_n+u^2J_{ij}A_{kl}\right| =1+u^2\,\mathrm{tr}\,\left[
J_{ij}A_{kl}\right]
\end{equation}
This can be seen by noting that $J_{ij}$ is a rank one matrix so the product 
$J_{ij}A_{kl}$ is rank one implying that the only nonzero eigenvalue of $%
J_{ij}A_{kl}$ is $\,\mathrm{tr}\,\left[ J_{ij}A_{kl}\right] $ since the
trace of a matrix is the sum of it's eigenvalues. Also, adding the identity
matrix to $J_{ij}A_{kl}$ just increases all of the eigenvalues by one. Thus,
the eigenvalues of $I_n+u^2J_{ij}A_{kl}$ are $1+u^2\,\mathrm{tr}\,\left[
J_{ij}A_{kl}\right] $ and $n-1$ ones, and since the product of the
eigenvalues is equal to the determinant the result is obtained. With this
result we can finish evaluating the integral 
\begin{eqnarray}
\left\langle \phi _k\right| \frac 1{r_{ij}}\left| \phi _l\right\rangle
&=&\frac 2{\sqrt{\pi }}\left\langle \phi _k\right. |\left. \phi
_l\right\rangle \int_0^\infty \left( 1+u^2\,\mathrm{tr}\,\left[
J_{ij}A_{kl}\right] \right) ^{-3/2}\,du  \nonumber \\
&=&\frac 2{\sqrt{\pi }}\left\langle \phi _k\right. |\left. \phi
_l\right\rangle \left( \,\mathrm{tr}\,\left[ J_{ij}A_{kl}\right] \right)
^{-1/2}
\end{eqnarray}

With the above single term result in mind we now proceed to a matrix form
for the potential energy operator and matrix elements. The potential energy
operator, after separation of center of mass, is usually written 
\begin{equation}
\hat V=\sum_i^n\frac{q_0q_i}{r_i}+\sum_i^n\sum_{i<j}\frac{q_iq_j}{r_{ij}}
\end{equation}
where $q_0$ is the charge at the origin, typically a heavy nucleus, and the $%
q_i$ are the charges of the remaining particles. This operator can be
written in the matrix form 
\begin{eqnarray}
\hat V &=&\,\mathrm{tr}\,\left[ Q\left( r_{ij}^2\right) ^{\left[ -1/2\right]
}\right]  \nonumber \\
&=&\left( \,\mathrm{vec}\,Q^{\prime }\right) ^{\prime }\left( \mathcal{R}_n%
\mathcal{F}_{n3}\,\mathrm{vec}\,rr^{\prime }\right) ^{\left[ -1/2\right] }
\end{eqnarray}
where $Q$ is an $n\times n$ upper triangular matrix of the charge products 
\begin{equation}
Q=\left( 
\begin{array}{cccc}
q_0q_1 & q_1q_2 & \cdots & q_1q_n \\ 
0 & q_0q_2 & q_2q_3 & q_2q_n \\ 
\vdots &  & \ddots & \vdots \\ 
0 & \cdots & 0 & q_0q_n
\end{array}
\right)
\end{equation}
and $\left( r_{ij}^2\right) ^{\left[ -1/2\right] }$ is the Hadamard, i.e.
term by term, reciprocal square root of the matrix $\left( r_{ij}^2\right) $
whose $ij^{th}$ element is $r_{ij}^2$, recall that the diagonal terms are
defined as in equation (\ref{rij2}). The basis matrix $\mathcal{F}_{n3}$ is
defined by its action on the vector $\,\mathrm{vec}\,rr^{\prime }$%
\begin{equation}
\mathcal{F}_{n3}\,\mathrm{vec}\,rr^{\prime }=\,\mathrm{vec}\,\left( r_i\cdot
r_j\right)
\end{equation}
That is, $\mathcal{F}_{n3}$ is the $n^2\times \left( 3n\right) ^2$ matrix
that transforms the vec of the $\left( 3n\right) ^2\times \left( 3n\right)
^2 $ matrix $rr^{\prime }$ into the vec of the $n^2\times n^2$ matrix whose $%
ij^{th}$ element is $r_i\cdot r_j$ where $r_i$ is the coordinate vector for
particle $i$. The basis matrix $\mathcal{R}_n$ is defined by 
\begin{equation}
\mathcal{R}_n\,\mathrm{vec}\,\left( r_i\cdot r_j\right) =\,\mathrm{vec}%
\,\left( r_{ij}^2\right)
\end{equation}
Both $\mathcal{F}_{nm}$ and $\mathcal{R}_n$ are basis matrices for linear
structures and are useful in their own right.

In general $\mathcal{R}_n$ is the $n^2\times n^2$ matrix that acts on the
vec of the arbitrary $n\times n$ matrix $\left( a_{ij}\right) $ as follows 
\begin{equation}
\mathcal{R}_n\,\mathrm{vec}\,\left( a_{ij}\right) =\,\mathrm{vec}\,\left(
\left\{ 
\begin{array}{ll}
a_{ii} & i=j \\ 
a_{ii}+a_{jj}-a_{ij}-a_{ji} & i\neq j
\end{array}
\right. \right)
\end{equation}
Note the similarity to the effects of the matrix $J_{ij}$ in equation (\ref
{Jij}). In fact the rows of $\mathcal{R}_n$ are precisely $\left( \,\mathrm{%
vec}\,J_{ij}\right) ^{\prime }$. This relation gives a possible construction
for $\mathcal{R}_n$. However, $\mathcal{R}_n$ can also be defined in terms
of the duplication matrix $\mathcal{D}_n$ and the matrix $\mathcal{N}_n$
from the section on linear structures 
\begin{equation}
\mathcal{R}_n=2\mathcal{N}_n\left( 1_n\otimes I_n\right) \left( 2\mathcal{N}%
_n-\mathcal{D}_n\mathcal{D}_n^{\prime }\right) -\mathcal{D}_n\mathcal{D}%
_n^{\prime }
\end{equation}
Here $1_n$ is an $n\times n$ matrix of $1$'s.

An interesting derivation of the transformation $\mathcal{F}_{n3}$ is in
terms of a matrix differential. Writing the quadratic form $r^{\prime
}\left( A\otimes I_3\right) r$ as $\sum_{ij}a_{ij}r_i\cdot r_j$ clearly
shows that the derivative with respect to a matrix element $a_{ij}$ would
give the term $r_i\cdot r_j$. Now, taking the differential of the matrix
function $r^{\prime }\left( A\otimes I_3\right) r$ with respect to the
matrix $A$ gives, 
\begin{eqnarray}
d\left( r^{\prime }\left( A\otimes I_3\right) r\right) &=&d\,\mathrm{tr}%
\,\left[ rr^{\prime }\left( A\otimes I_3\right) \right]  \nonumber \\
\ &=&\,\mathrm{tr}\,\left[ rr^{\prime }\left( dA\otimes I_3\right) \right] 
\nonumber \\
\ &=&\left( \,\mathrm{vec}\,rr^{\prime }\right) ^{\prime }\,\mathrm{vec}%
\,\left( dA\otimes I_3\right)  \nonumber \\
\ &=&\left( \,\mathrm{vec}\,rr^{\prime }\right) ^{\prime }\left( I_n\otimes 
\mathcal{K}_{3n}\otimes I_3\right) \left( \,\mathrm{vec}\,dA\otimes \,%
\mathrm{vec}\,I_3\right)  \nonumber \\
\ &=&\left( \,\mathrm{vec}\,rr^{\prime }\right) ^{\prime }\left( I_n\otimes
\left( \mathcal{K}_{3n}\otimes I_3\right) \left( I_n\otimes \mathrm{vec}%
\,I_3\right) \right) d\mathrm{vec}A
\end{eqnarray}
Thus, the gradient of $r^{\prime }\left( A\otimes I_3\right) r$ with respect
to $\,\mathrm{vec}\,A$ is 
\begin{equation}
\left( I_n\otimes \left( \mathcal{K}_{3n}\otimes I_3\right) \left(
I_n\otimes \,\mathrm{vec}\,I_3\right) \right) ^{\prime }\left( \,\mathrm{vec}%
\,rr^{\prime }\right) =\mathcal{F}_{n3}\,\mathrm{vec}\,rr^{\prime }
\end{equation}
In general $\mathcal{F}_{nm}=\left( I_n\otimes \left( \mathcal{K}%
_{3n}\otimes I_m\right) \left( I_n\otimes \,\mathrm{vec}\,I_m\right) \right)
^{\prime }$ with dimensions $n^2\times \left( mn\right) ^2$. An unrelated
but useful property of $\mathcal{F}_{nm}$ is 
\begin{equation}
\mathcal{F}_{nm}\,\mathrm{vec}\,\left( A\otimes B\right) =\,\mathrm{tr}%
\,\left[ B\right] \,\mathrm{vec}\,A
\end{equation}
for square matrices $A,\;n\times n,$ and $B,\;m\times m,$ this is
essentially a factorization of the Kronecker product and could be useful in
solving certain matrix equations.

With all of these results in place, the potential energy integral can be
written in matrix form as, 
\begin{equation}
\left\langle \phi _k\right| \left( \,\mathrm{vec}\,Q^{\prime }\right)
^{\prime }\left( \mathcal{R}_n\mathcal{F}_{n3}\,\mathrm{vec}\,rr^{\prime
}\right) ^{\left[ -1/2\right] }\left| \phi _l\right\rangle =\frac 2{\sqrt{%
\pi }}\left\langle \phi _k\right. |\left. \phi _l\right\rangle \left( \,%
\mathrm{vec}\,Q^{\prime }\right) ^{\prime }\left( \mathcal{R}_n\,\mathrm{vec}%
\,A_{kl}^{-1}\right) ^{\left[ -1/2\right] }
\end{equation}
Thus, using normalized basis functions, the potential energy matrix
elements, $V_{lk}$ are 
\begin{equation}
V_{kl}=\frac 2{\sqrt{\pi }}S_{kl}\left( \,\mathrm{vec}\,Q^{\prime }\right)
^{\prime }\left( \mathcal{R}_n\,\mathrm{vec}\,A_{kl}^{-1}\right) ^{\left[
-1/2\right] }  \label{Vkl}
\end{equation}

\section{Gradient of the Energy Functional}

Quantum chemical calculations using this basis set are typically variational
in nature. An approximation to an exact wave function is written as a finite
expansion of correlated Gaussians, the secular equation is constructed, and
a selected energy eigenvalue is minimized with respect to the linear
expansion coefficients and the independent nonlinear parameters contained in
the matrices $L_k$ of the individual basis functions. Solution of the
resulting eigensystem automatically determines the optimal linear expansion
coefficients as the eigenvectors for a given set of the nonlinear
parameters, but optimization of the nonlinear parameters is a very difficult
and computationally expensive task. The first order necessary conditions for
an energy minimum require the energy gradient (equivalently differential or
derivative) with respect to all of the matrices $L_k$ to vanish. This
section begins the construction of this gradient in an exact analytic form.

The secular equation 
\begin{equation}
\left( H-\epsilon S\right) c=0
\end{equation}
defines $\epsilon $ as an implicit function of the $N\times N$ matrices $H$
and $S$. $H$ and $S$ are themselves functions of the $Nn\left( n+1\right)
/2\times 1$ vector $a$ of nonlinear exponential parameters contained in the
matrices $L_k,$ equation(\ref{avec}).

Taking the differential of the secular equation gives 
\begin{eqnarray}
d\left[ \left( H-\epsilon S\right) c\right] &=&d\left( H-\epsilon S\right)
c+\left( H-\epsilon S\right) \,dc  \nonumber \\
\ &=&\left( dH-\left( d\epsilon \right) S-\epsilon \,dS\right) c+\left(
H-\epsilon S\right) \,dc
\end{eqnarray}
multiplying from the left by $c^{\prime },$ rearranging, and noting $%
c^{\prime }\left( H-\epsilon S\right) =0$ results in 
\begin{equation}
d\epsilon =\frac{c^{\prime }\left( dH-\epsilon \,dS\right) c}{c^{\prime }Sc}
\end{equation}
This is the differential with respect to the matrices $H$ and $S$. For the
rest of this derivation the scalar term $c^{\prime }Sc$ will be dropped from
the calculations for convenience. 
\begin{eqnarray}
d\epsilon &=&\,\mathrm{tr}\,\left[ cc^{\prime }\left( dH-\epsilon
\,dS\right) \right]  \nonumber \\
\ &=&\left( \,\mathrm{vec}\,cc^{\prime }\right) ^{\prime }\,\mathrm{vec}%
\,\left( dH-\epsilon \,dS\right)  \nonumber \\
\ &=&\left( \,\mathrm{vec}\,cc^{\prime }\right) ^{\prime }\mathcal{D}_n\,%
\mathrm{vech}\,\left( dH-\epsilon \,dS\right)  \nonumber \\
\ &=&\left( \mathcal{D}_n^{\prime }\mathrm{vec}\,cc^{\prime }\right)
^{\prime }\left( d\,\mathrm{vech}\,H-\epsilon \,d\,\mathrm{vech}\,S\right) 
\nonumber \\
\ &=&\left( \,\mathrm{vech}\,\left[ 2cc^{\prime }-\,\mathrm{diag}%
\,cc^{\prime }\right] \right) ^{\prime }\left( \frac{\partial \,\mathrm{vech}%
\,H}{\partial a^{\prime }}-\epsilon \frac{\partial \,\mathrm{vech}\,S}{%
\partial a^{\prime }}\right) \,da
\end{eqnarray}
Bringing back the scalar term $c^{\prime }Sc$ the energy gradient with
respect to $a$ is 
\begin{equation}
\nabla _a\epsilon =\frac 1{c^{\prime }Sc}\left( \frac{\partial \,\mathrm{vech%
}\,H}{\partial a^{\prime }}-\epsilon \frac{\partial \,\mathrm{vech}\,S}{%
\partial a^{\prime }}\right) ^{\prime }\left( \,\mathrm{vech}\,\left[
2cc^{\prime }-\,\mathrm{diag}\,cc^{\prime }\right] \right)
\end{equation}

The matrix $\left( \partial \,\mathrm{vech}\,H/\partial a^{\prime }-\epsilon
\,\,\partial \,\mathrm{vech}\,S/\partial a^{\prime }\right) $ in the
gradient above has dimension $N\left( N+1\right) /2\times Nn\left(
n+1\right) /2$. It is also sparse with $N^2n\left( n+1\right) \left(
N+1\right) /4$ total elements, $N^2n\left( n+1\right) /2$ non-zero elements
and $N^2n\left( N-1\right) \left( n+1\right) /4$ zeros. The non-zero terms
in the matrices $\partial \,\mathrm{vech}\,H/\partial a^{\prime }$ and $%
\partial \,\mathrm{vech}\,S/\partial a^{\prime }$ are contained in the $%
1\times n\left( n+1\right) /2$ vectors $\partial \,H_{kl}/\partial \left( \,%
\mathrm{vech}\,L_k\right) ^{\prime }$ and $\partial \,S_{kl}/\partial \left(
\,\mathrm{vech}\,L_k\right) ^{\prime }$ equivalently \\$\partial
\,H_{kl}/\partial \left( \,\mathrm{vech}\,L_l\right) ^{\prime }$ and $%
\partial \,S_{kl}/\partial \left( \,\mathrm{vech}\,L_l\right) ^{\prime }$.
In the next section of this paper we find these derivatives.

\section{First Derivatives}

The power of the matrix differential calculus is immediately apparent when
one actually computes an analytic gradient for a matrix function. The ease
with which results are obtained and the concise compact form of the results
seems almost miraculous at times. When the derivatives of this paper where
first formulated the results where so surprising that numerical conformation
was performed immediately. All of the following matrix derivatives have been
confirmed by finite differences term by term on random matrices. The
following derivations should not be difficult to follow if the preliminary
matrix results presented at the beginning of this paper are within easy
reach. The derivatives are all with respect to the independent parameters
contained in the matrix $L_k$ i.e. $\,\mathrm{vech}\,L_k$.

\subsection{Overlap}

Beginning with the function $\left\langle \phi _k\right. |\left. \phi
_l\right\rangle =\pi ^{3n/2}\left| A_{kl}\right| ^{-3/2}$, the differential
is 
\begin{eqnarray}
d\left\langle \phi _k\right. |\left. \phi _l\right\rangle &=&\pi
^{3n/2}\left| A_{kl}\right| ^{-5/2}d\left| A_{kl}\right|  \nonumber \\
\ &=&-\frac 32\pi ^{3n/2}\left| A_{kl}\right| ^{-5/2}\left| A_{kl}\right| \,%
\mathrm{tr}\,\left[ A_{kl}^{-1}\,dA_{kl}\right]  \nonumber \\
\ &=&-\frac 32\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,%
\mathrm{tr}\,\left[ A_{kl}^{-1}\,d\left( L_kL_k^{\prime }+L_lL_l^{\prime
}\right) \right]  \nonumber \\
\ &=&-\frac 32\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,%
\mathrm{tr}\,\left[ A_{kl}^{-1}\left( \left( dL_k\right) L_k^{\prime
}+L_k\,dL_k^{\prime }\right) \right]  \nonumber \\
\ &=&-3\left\langle \phi _k\right. |\left. \phi _l\right\rangle \,\mathrm{tr}%
\,\left[ L_k^{\prime }A_{kl}^{-1}\,dL_k\right]  \nonumber \\
\ &=&-3\left\langle \phi _k\right. |\left. \phi _l\right\rangle \left( \,%
\mathrm{vec}\,A_{kl}^{-1}L_k\right) ^{\prime }\,\mathrm{vec}\,dL_k  \nonumber
\\
\ &=&-3\left\langle \phi _k\right. |\left. \phi _l\right\rangle \left( \,%
\mathrm{vec}\,A_{kl}^{-1}L_k\right) ^{\prime }\mathcal{L}_n^{\prime }\,%
\mathrm{vech}\,dL_k  \nonumber \\
\ &=&-3\left\langle \phi _k\right. |\left. \phi _l\right\rangle \left( 
\mathcal{L}_n\,\mathrm{vec}\,A_{kl}^{-1}L_k\right) ^{\prime }d\,\mathrm{vech}%
\,L_k  \nonumber \\
\ &=&-3\left\langle \phi _k\right. |\left. \phi _l\right\rangle \left( \,%
\mathrm{vech}\,A_{kl}^{-1}L_k\right) ^{\prime }d\,\mathrm{vech}\,L_k
\end{eqnarray}
therefore, 
\begin{equation}
\frac{\partial \left\langle \phi _k\right. |\left. \phi _l\right\rangle }{%
\partial \left( \,\mathrm{vech}\,L_k\right) ^{\prime }}=-3\left\langle \phi
_k\right. |\left. \phi _l\right\rangle \left( \,\mathrm{vech}%
\,A_{kl}^{-1}L_k\right) ^{\prime }
\end{equation}
The derivative of the normalized overlap matrix element, $S_{kl}$, equation (%
\ref{skl}), presents no new challenges and it is a simple exercise to show 
\begin{equation}
\frac{\partial S_{kl}}{\partial \left( \,\mathrm{vech}\,L_k\right) ^{\prime }%
}=\frac 32S_{kl}\left( \,\mathrm{vech}\,\left[ \left( L_k^{-1}\right)
^{\prime }-2A_{kl}^{-1}L_k\right] \right) ^{\prime }
\end{equation}

\subsection{Kinetic Energy}

To find the derivative of the kinetic energy matrix element first note that $%
A_kA_{kl}^{-1}A_l$ way be written as 
\begin{equation}
A_kA_{kl}^{-1}A_l=\left( A_k^{-1}+A_l^{-1}\right) ^{-1}
\end{equation}
and 
\begin{equation}
dS_{kl}=\frac 32S_{kl}\,\mathrm{tr}\,\left[ \left( L_k^{-1}-2L_k^{\prime
}A_{kl}^{-1}\right) \,dL_k\right]
\end{equation}
The differential of the kinetic energy matrix element $T_{kl}=6S_{kl}\,%
\mathrm{tr}\,\left[ MA_kA_{kl}A_l\right] $ is 
\begin{eqnarray}
dT_{kl} &=&6\,dS_{kl}\,\mathrm{tr}\,\left[ MA_kA_{kl}A_l\right] +6S_{kl}\,d\,%
\mathrm{tr}\,\left[ MA_kA_{kl}A_l\right]  \nonumber \\
\ &=&9S_{kl}\,\mathrm{tr}\,\left[ \left( L_k^{-1}-2L_k^{\prime
}A_{kl}^{-1}\right) \,dL_k\right] \,\mathrm{tr}\,\left[ MA_kA_{kl}A_l\right]
+  \nonumber \\
&&6S_{kl}\,\mathrm{tr}\,\left[ M\,d\left( A_k^{-1}+A_l^{-1}\right)
^{-1}\right]  \nonumber \\
\ &=&\frac 32T_{kl}\,\mathrm{tr}\,\left[ \left( L_k^{-1}-2L_k^{\prime
}A_{kl}^{-1}\right) \,dL_k\right] +  \nonumber \\
&&\ \,\,\,6S_{kl}\,\mathrm{tr}\,\left[ M\left( A_k^{-1}+A_l^{-1}\right)
^{-1}A_k^{-1}\left( dA_k\right) A_k^{-1}\left( A_k^{-1}+A_l^{-1}\right)
^{-1}\right]  \nonumber \\
\ &=&\frac 32T_{kl}\,\mathrm{tr}\,\left[ \left( L_k^{-1}-2L_k^{\prime
}A_{kl}^{-1}\right) \,dL_k\right] +6S_{kl}\,\mathrm{tr}\,\left[
MA_lA_{kl}^{-1}\left( dA_k\right) A_{kl}^{-1}A_l\right]  \nonumber \\
\ &=&\frac 32T_{kl}\,\mathrm{tr}\,\left[ \left( L_k^{-1}-2L_k^{\prime
}A_{kl}^{-1}\right) \,dL_k\right] +12S_{kl}\,\mathrm{tr}\,\left[ L_k^{\prime
}A_{kl}^{-1}A_lMA_lA_{kl}^{-1}\,dL_k\right]  \nonumber \\
\ &=&\left( \,\mathrm{vech}\,\left[ \frac 32T_{kl}\left( \left(
L_k^{-1}\right) ^{\prime }-2A_{kl}^{-1}L_k\right)
+12S_{kl}A_{kl}^{-1}A_lMA_lA_{kl}^{-1}L_k\right] \right) ^{\prime }d\,%
\mathrm{vech}\,L_k  \nonumber \\
&&\,\,
\end{eqnarray}
whence 
\begin{equation}
\frac{\partial T_{kl}}{\partial \left( \,\mathrm{vech}\,L_k\right) ^{\prime }%
}=\left( \,\mathrm{vech}\,\left[ \frac 32T_{kl}\left( \left( L_k^{-1}\right)
^{\prime }-2A_{kl}^{-1}L_k\right) +12S_{kl}\left(
A_{kl}^{-1}A_lMA_lA_{kl}^{-1}L_k\right) \right] \right) ^{\prime }
\end{equation}

\subsection{Potential Energy}

The formula for the potential energy uses a Hadamard power, because of
this,equation (\ref{vechad}) will be needed. The differential of 
\[
V_{kl}=\frac 2{\sqrt{\pi }}S_{kl}\left( \,\mathrm{vec}\,Q^{\prime }\right)
^{\prime }\left( \mathcal{R}_n\,\mathrm{vec}\,A_{kl}^{-1}\right) ^{\left[
-1/2\right] } 
\]
is then 
\begin{eqnarray}
dV_{kl} &=&\frac 32V_{kl}\,\mathrm{tr}\,\left[ \left( L_k^{-1}-2L_k^{\prime
}A_{kl}^{-1}\right) \,dL_k\right] +\frac 2{\sqrt{\pi }}S_{kl}\left( \,%
\mathrm{vec}\,Q^{\prime }\right) ^{\prime }\,d\left( \mathcal{R}_n\,\mathrm{%
vec}\,A_{kl}^{-1}\right) ^{\left[ -1/2\right] }  \nonumber \\
\ &=&\frac 32V_{kl}\left( \,\mathrm{vec}\,\left[ \left( L_k^{-1}\right)
^{\prime }-2A_{kl}^{-1}L_k\right] \right) ^{\prime }\,d\,\mathrm{vec}\,L_k+ 
\nonumber \\
&&\frac 2{\sqrt{\pi }}S_{kl}\left( \,\mathrm{vec}\,Q^{\prime }\right)
^{\prime }\left( -\frac 12\right) \left( \mathcal{R}_n\,\mathrm{vec}%
\,A_{kl}^{-1}\right) ^{\left[ -3/2\right] }\odot \mathcal{R}_n\,\mathrm{vec}%
\,\left[ -A_{kl}^{-1}\left( dA_{kl}\right) A_{kl}^{-1}\right]  \nonumber \\
\ &=&\frac 32V_{kl}\left( \,\mathrm{vech}\,\left[ \left( L_k^{-1}\right)
^{\prime }-2A_{kl}^{-1}L_k\right] \right) ^{\prime }\,d\,\mathrm{vech}\,L_k+
\nonumber \\
&&\frac 1{\sqrt{\pi }}S_{kl}\left( \,\mathrm{vec}\,Q^{\prime }\odot \left( 
\mathcal{R}_n\,\mathrm{vec}\,A_{kl}^{-1}\right) ^{\left[ -3/2\right]
}\right) ^{\prime }\mathcal{R}_n\left( A_{kl}^{-1}\otimes A_{kl}^{-1}\right)
\,\mathrm{vec}\,dA_{kl}  \label{dVkl1}
\end{eqnarray}
To find the differential $\,\mathrm{vec}\,dA_{kl}$ the commutation matrix is
used 
\begin{eqnarray}
\,\mathrm{vec}\,dA_{kl} &=&\,\mathrm{vec}\,dA_k=\,\mathrm{vec}\,\left[
\left( dL_k\right) L_k^{\prime }\right] +\,\mathrm{vec}\,\left[
L_k\,dL_k^{\prime }\right]  \nonumber \\
\ &=&\left( L_k\otimes I_n\right) \,\mathrm{vec}\,dL_k+\left( I_n\otimes
L_k\right) \mathcal{K}_{nn}\,\mathrm{vec}\,dL_k  \nonumber \\
\ &=&\left( I_n+\mathcal{K}_{nn}\right) \left( L_k\otimes I_n\right) \,%
\mathrm{vec}\,dL_k  \nonumber \\
\ &=&2\mathcal{N}_n\left( L_k\otimes I_n\right) \,\mathrm{vec}\,dL_k
\end{eqnarray}
Using this result in (\ref{dVkl1}) gives 
\begin{equation}
dV_{kl}=\frac 32V_{kl}\left( \,\mathrm{vech}\,\left[ \left( L_k^{-1}\right)
^{\prime }-2A_{kl}^{-1}L_k\right] \right) ^{\prime }\,d\,\mathrm{vech}\,L_k+
\end{equation}
\[
\frac 2{\sqrt{\pi }}S_{kl}\left( \,\mathrm{vec}\,Q^{\prime }\odot \left( 
\mathcal{R}_n\,\mathrm{vec}\,A_{kl}^{-1}\right) ^{\left[ -3/2\right]
}\right) ^{\prime }\mathcal{R}_n\left( A_{kl}^{-1}\otimes A_{kl}^{-1}\right) 
\mathcal{N}_n\left( L_k\otimes I_n\right) \mathcal{L}_n^{\prime }\,d\,%
\mathrm{vech}\,L_k 
\]
Now, note that $\left( A\otimes A\right) \mathcal{N}_n=\mathcal{N}_n\left(
A\otimes A\right) $ and $\mathcal{R}_n\mathcal{N}_n=\mathcal{R}_n$. With
these identities the derivative of the potential energy can be written as 
\[
\frac{\partial V_{kl}}{\partial \left( \,\,\mathrm{vech}\,L_k\right)
^{\prime }}=\frac 32V_{kl}\left( \,\mathrm{vech}\,\left[ \left(
L_k^{-1}\right) ^{\prime }-2A_{kl}^{-1}L_k\right] \right) ^{\prime }+ 
\]
\begin{equation}
\frac 2{\sqrt{\pi }}S_{kl}\left( \,\mathrm{vec}\,Q^{\prime }\odot \left( 
\mathcal{R}_n\,\mathrm{vec}\,A_{kl}^{-1}\right) ^{\left[ -3/2\right]
}\right) ^{\prime }\mathcal{R}_n\left( A_{kl}^{-1}L_k\otimes
A_{kl}^{-1}\right) \mathcal{L}_n^{\prime }
\end{equation}
This completes the derivation of the derivatives needed for the gradient of
the energy functional.

A few final notes should be made regarding diagonal matrix elements and
derivatives. First there is significant simplification in the integral and
derivative formulas given above when $k=l$. Writing separate code to compute
diagonal matrix elements and derivatives is thus warranted. Lastly it should
be noted that $\,\mathrm{vech}\,L_k$ and $\,\mathrm{vech}\,L_l$ are
independent vector variables, thus, $\partial H_{kk}/\left( \partial \,%
\mathrm{vech}\,L_k\right) ^{\prime }$ is not just $\partial H_{kl}/\left(
\partial \,\mathrm{vech}\,L_k\right) ^{\prime }$ evaluated at $k=l$. In
fact, due to the symmetry between $k$ and $l$ in the integral formulas $%
\partial H_{kk}/\left( \partial \,\mathrm{vech}\,L_k\right) ^{\prime
}=2\times \left. \left( \partial H_{kl}/\left( \partial \,\mathrm{vech}%
\,L_k\right) ^{\prime }\right) \right| _{k=l}$. Formulas for integrals and
derivatives when $k=l$ should be derived and coded separately from the $%
k\neq l$ case.

\section{Conclusion}

This work has utilized a new and powerful matrix differential calculus for
new matrix derivations of integrals and derivatives involving correlated
Gaussian functions including a complete analytic gradient for the energy
functional. New vector/matrix forms for the kinetic and potential energy
operators were presented. The compact matrix forms of these results can be
manipulated using matrix algebra and are readily implemented using optimized
computer subroutine libraries.

The results in this work will be used for variational calculations on
atomic, positron, and muonic systems. The matrix methods presented in this
work will also be applied to extension of the correlated Gaussian basis set
to multicenter functions, functions representing higher angular momentum
states, functions with premultipliers etc..

It is expected that the results in this work will be useful to those working
with correlated Gaussian functions. Furthermore, and more importantly, it is
hoped that this presentation and use of the matrix differential calculus
will inspire its use for other problems in quantum chemistry.

\chapter{Implementation of Gradient Formulas for Correlated Gaussians: $^4$%
He, $^\infty $He, Ps$_2$, $^9$Be, and $^\infty $Be Test Results}

\newpage 

\section{Introduction}

Explicitly correlated Gaussian basis functions, despite their relative
simplicity compared to other explicitly correlated bases and despite the
enhanced convergence they exhibit, are not so widely used as orbital product
wave functions. This is due to several difficulties that have plagued their
use since their introduction 35 years ago. Not the least among these have
been the labyrinthine formulas for evaluating matrix elements and gradients,
and the formidable problem to optimize the many exponential variational
parameters in this basis. The former problem was addressed, and largely
solved, by application of matrix differential calculus described in a
previous paper (hereafter referred to as paper 1\cite{Kinghorn95a}). The
latter problem remains a serious stumbling block although, as demonstrated
in the present work, the analytic gradient formulas of paper 1\cite
{Kinghorn95a} significantly reduce optimization costs.

With rare exceptions, past applications of explicitly correlated Gaussian
basis functions were left incompletely optimized, often suggesting much
slower convergence than this basis is capable of. Much clever but
essentially \textit{ad hoc} experimentation has been done to solve the
optimization problem. The number of parameters has been reduced using
various tempering schemes. Nearly every new optimization strategy has been
tried, some of which do not require gradients and many which do. Often
numerical approximations for gradients were used with consequent increase in
optimization costs. In the present paper we achieve much more thorough
optimization with large basis sets than we could achieve previously without
the advantage of efficient formulas for analytic gradients.

The analytic formulas derived in paper 1 have been coded for general
n-particle systems and tested for the following 3-, 4-, and 5-particle
systems: $^4$He, $^\infty $He, Ps$_2$, $^9$Be, and $^\infty $Be. Thorough
optimization has been achieved with 4, 8, and 16 basis functions in each
case, and partial optimization for up to 128 basis functions. In every case
we were able to surpass the best previously reported bound with the same
size basis.

This paper is organized as follows. A brief section introduces the essential
matrix calculus notation. Then the center of mass transformation and
internal kinetic and potential energy operators are defined. The correlated
Gaussian basis functions and the non-linear exponential parameters are
introduced. Particle permutation operators are defined and transformed to
the center of mass frame of reference. Projectors for requisite
permutational symmetry states are derived for each of the test cases to be
studied. Matrix elements over symmetry projected wave functions are then
derived. Next, the components of the energy gradient for symmetry projected
basis functions are given. Physical characteristics of the test cases are
given. Hardware, algebraic and optimization software are then described.
Timing results are reported for the optimization subroutines and for
gradient and energy evaluations, followed by the newly optimized energies of
each test case. Finally, we discuss these results and draw conclusions.

\section{Notation}

Some of the notation and matrix constructs used in this work may be
unfamiliar to some readers. A more complete description of notation and
matrix methods can be found in paper 1 and the references therein. Here a
few important definitions are given, other definitions are included
throughout the text as needed.

\subsection{The vec and vech Operators}

The vec operator transforms a matrix into a vector by stacking the columns
of the matrix one underneath the other.

Let $A$ be an $m\times n$ matrix and $a_j$ its $j^{th}$ column, then $\,%
\mathrm{vec}\,A$ is the $mn\times 1$ vector 
\begin{equation}
\,\mathrm{vec}\,A=\left[ 
\begin{array}{c}
a_1 \\ 
a_2 \\ 
\vdots \\ 
a_n
\end{array}
\right]
\end{equation}
If the original dimensions of $A$ are known then the vec operation is
invertible and the $ij^{th}$ element of $A$ is given by the indexing scheme $%
a_{ij}=\left( \,\mathrm{vec}\,A\right) _{i+\left( j-1\right) m}$.

An operator similar to vec is the vech, ``vector half'', operator. Let $A$
be a square $n\times n$ matrix . Then $\,\mathrm{vech}\,A$ is the $n\left(
n+1\right) /2\times 1$ vector obtained by stacking the lower triangular
elements of $A$. For example, if $n=3$, 
\begin{equation}
\,\mathrm{vech}\,A=\left[ 
\begin{array}{c}
a_{11} \\ 
a_{21} \\ 
a_{31} \\ 
a_{22} \\ 
a_{32} \\ 
a_{33}
\end{array}
\right]
\end{equation}
For symmetric $X$, $\,\mathrm{vech}\,X$ contains the independent elements of 
$X$. For a review of the notation, properties, and some applications of the
vec and vech operators see the papers by Henderson and Searle\cite
{HendersonSearle79,HendersonSearle80}.

\subsection{Basis matrices}

Basis matrices arise in the description of linear structures given by Magnus%
\cite{Magnus88}, and are described in paper1. Here basis matrices can be
thought of simply as transformation matrices on $\,\mathrm{vec}\,A$ or $\,%
\mathrm{vech}\,A$. Three basis matrices are used in this paper, $\mathcal{R}%
_n,\;\mathcal{F}_{n3},$ and $\mathcal{L}_n$. The first two, $\mathcal{R}_n,\;%
\mathcal{F}_{n3},$ are defined along with the new matrix/vector formula for
the potential energy operator. $\mathcal{L}_n$, known as the elimination
matrix is defined operationally by $\mathcal{L}_n\,\mathrm{vec}\,L=\,\mathrm{%
vech}\,L$ for any lower triangular matrix $L.$ Again the reader is urged to
consult paper 1 for more details.

\subsection{Matrix derivatives}

For $F\left( X\right) $ an $m\times n$ matrix function of the $p\times q$
matrix variable $X$, the derivative or Jacobian of $F$ with respect to $X\,$
is unambiguously defined as the $mn\times pq$ matrix of partial derivatives 
\begin{equation}
\frac{\partial \,\mathrm{vec}\,F\left( X\right) }{\partial \left( \,\mathrm{%
vec}\,X\right) ^{\prime }}  \label{derdef}
\end{equation}
whose $ij^{th}$ element is the partial derivative of the $i^{th}$ component
of $\,\mathrm{vec}\,F\left( X\right) $, a column vector, with respect to the 
$j^{th}$ element of $\left( \,\mathrm{vec}\,X\right) ^{\prime },$ a row
vector. This is the form of the matrix derivative defined by Magnus and
Neudecker\cite{MagNeud88} and used in paper 1.

\subsection{Other notation}

\begin{itemize}
\item  The Kronecker product of two matrices $A$ and $B$ is denoted by $%
A\otimes B.$

\item  The Hadamard product, element by element product, of two matrices is
denoted by $A\odot B$ and defined by $\left( A\odot B\right)
_{ij}=a_{ij}b_{ij}$ when $A$ and $B$ are the same size.

\item  The Hadamard power was defined in paper 1 as $A^{\left[ \alpha
\right] }=\left( a_{ij}^\alpha \right) $ when $a_{ij}^\alpha $ is well
defined for all of the $ij$ elements in $A.$

\item  $\,\mathrm{diag}\,A$ is the diagonal $n\times n$ matrix consisting of
the diagonal elements of the square matrix $A.$

\item  The trace of $A$ is denoted $\,\mathrm{tr}\,A$

\item  The transpose of $A$ is denoted $A^{\prime }$
\end{itemize}

\subsection{Units}

Throughout this work atomic units are used such that $m_e=1$ , the electron
charge is -1, and $\hslash =1$. Thus, energy is in hartrees, $%
E_h=27.2107eV=2.19474\times 10^5cm^{-1}$.

\section{The Hamiltonian}

Consider a system of $p$ particles with masses $\{M_1,\cdots ,M_p\}$ and
charges $\{Q_1,\cdots ,Q_p\}$ interacting under a coulomb potential. The
nonrelativistic Hamiltonian can be transformed from real particle
coordinates, column vector $R$, to center of mass, $r_{0\text{, }}$ and
internal, $r$, (pseudo particle) coordinates under the transformation $%
T:R\mapsto [r_0^{\prime },r^{\prime }]^{\prime }$ given by 
\begin{equation}
T=\left[ 
\begin{array}{ccccc}
\frac{M_1}{m_0} & \frac{M_2}{m_0} & \frac{M_3}{m_0} & \cdots & \frac{M_N}{m_0%
} \\ 
-1 & 1 & 0 & \cdots & 0 \\ 
-1 & 0 & 1 & \cdots & 0 \\ 
\vdots & \vdots & \vdots & \ddots & \vdots \\ 
-1 & 0 & 0 & \cdots & 1
\end{array}
\right] \otimes I_3  \label{tranT}
\end{equation}
Here $R$ is the length $3p$ column vector of real particle coordinates, $r$
is an $3n=3\left( p-1\right) $ column vector of pseudo particle coordinates, 
$m_0=\sum_i^pM_i$ and $I_3$ is the $3\times 3$ identity matrix. Details of
the center of mass transformation can be found in Kinghorn\cite{Kinghorn93}.
Note: the transformation matrix $T$ is also used to transform real particle
permutation matrices to pseudo particle permutation matrices.

Under the transformation $T$ the total Hamiltonian becomes $%
H_{tot}=H_{c.m.}+H_{int}$ where $H_{c.m.}$ represents the translational
kinetic energy of the center of mass and $H_{int}$ is the Hamiltonian for
the internal energy of the interacting particles. The discrete eigenstates
of the internal Hamiltonian are considered in this work.

The internal Hamiltonian is usually written in the familiar form $H=\hat{T}+%
\hat{V}$ with the kinetic energy operator $\hat{T},$ including mass
polarization terms, given by 
\begin{equation}
\hat{T}=-\frac 12\left( \sum_i^n\frac 1{\mu _i}\nabla _i^2+\sum_{i,j}^n\frac
1{M_1}\nabla _i\cdot \nabla _j\right)  \label{oldT}
\end{equation}
and 
\begin{equation}
\hat{V}=\sum_i^n\frac{q_0q_i}{r_i}+\sum_i^n\sum_{i<j}\frac{q_iq_j}{r_{ij}}
\label{oldV}
\end{equation}
Here $\mu _i=M_1M_i/\left( M_1+M_i\right) $ is the reduced mass of (pseudo)
particle $i$, $M_1$ is the mass at the coordinate origin, $\nabla _i$ is the
gradient operator with respect to the $x,y,z$ coordinates of particle $i$, $%
q_0=Q_1,\,q_1=Q_2,\ldots $, and $r_{ij}=\left\| r_i-r_j\right\| $ where $r_i$
is the Cartesian coordinate vector for particle $i$.

It was shown in paper 1 that this internal Hamiltonian can be written in
matrix/vector form with 
\begin{equation}
\hat{T}=-\nabla _r\cdot \left( M\otimes I_3\right) \nabla _r  \label{newT}
\end{equation}
and 
\begin{equation}
\hat{V}=\left( \,\mathrm{vec}\,\left[ Q^{\prime }\right] \right) ^{\prime
}\left( \mathcal{R}_n\mathcal{F}_{n3}\,\mathrm{vec}\,\left[ rr^{\prime
}\right] \right) ^{\left[ -1/2\right] }  \label{newV}
\end{equation}
Here $M$ is an $n\times n$ matrix with $1/2\mu _i$ on the diagonal and $%
1/2M_1$ for the off diagonal entries, $\nabla _r$ is the gradient operator
with respect to $r$ the vector of all (pseudo) particle coordinates. In the
potential energy operator $Q$ is an $n\times n$ upper triangular matrix, 
\begin{equation}
Q=\left( 
\begin{array}{cccc}
q_0q_1 & q_1q_2 & \cdots & q_1q_n \\ 
0 & q_0q_2 & q_2q_3 & q_2q_n \\ 
\vdots &  & \ddots & \vdots \\ 
0 & \cdots & 0 & q_0q_n
\end{array}
\right)
\end{equation}
$\mathcal{R}_n$ is a basis matrix defined by $\mathcal{R}_n\,\mathrm{vec}%
\,\left[ \left( r_i\cdot r_j\right) \right] =\,\mathrm{vec}\,\left[ \left(
r_{ij}^2\right) \right] $ where, $\left( r_i\cdot r_j\right) $ and $\left(
r_{ij}^2\right) $ are $3n\times 3n$ matrices with the corresponding $ij$
elements. $\mathcal{F}_{3n}\,$ is defined by $\mathcal{F}_{3n}\,\mathrm{vec}%
\,\left[ rr^{\prime }\right] =\,\mathrm{vec}\,\left[ \left( r_i\cdot
r_j\right) \right] $. The reader is urged to read the paper 1 for more
details on the derivation of \ref{newT},\ref{newV} and the basis matrices $%
\mathcal{R}_n$ and $\mathcal{F}_{n3}$.

\section{Explicitly Correlated Gaussian Basis Functions}

The basis functions used in this work consist of negative expontentials of
positive definite quadratic forms. Using the notation from paper 1 
\begin{eqnarray}
\phi _k &=&\exp \left[ -r^{\prime }\left( L_kL_k^{\prime }\otimes I_3\right)
r\right]  \label{phik} \\
&=&\exp \left[ -r^{\prime }\left( A_k\otimes I_3\right) r\right]
\label{phiAk} \\
&=&\exp \left[ -r^{\prime }\bar{A}_kr\right]
\end{eqnarray}
where $r$ is a $3n\times 1$ vector of Cartesian coordinates for the $n$
particles, $L_k$ is an $n\times n$ rank $n$ lower triangular matrix. $k$
ranges from 1 to $N$ where $N$ is the number of basis functions. $%
A_k=L_kL_k^{\prime }$ is written in this Cholesky factored form to assure
positive definiteness of the quadratic form.

Correlation in the basis is achieved by including terms of the form $%
a_{ij}r_i\cdot r_j$ in the quadratic form, i.e. $\exp \left[ -r^{\prime
}\left( A_k\otimes I_3\right) r\right] =\exp \left[
-\sum_{i,j}a_{ij}r_i\cdot r_j\right] $. This is perhaps easier to see by
noting the identity, $r_{ij}^2=r_i\cdot r_i+r_j\cdot r_j-2r_i\cdot r_j$. In
this sense the $\phi _k$ contain information on all interparticle distances
and are thus explicitly correlated.

The Kronecker product with the $3\times 3$ identity matrix $I_3$ in equation
(\ref{phiAk}) insures rotational invariance of the basis functions. The $%
\phi _k$ are angular momentum eigenfunctions with $J=0$. Adaptation to
permutational symmetry is described in the next section.

\section{Symmetry}

Any coordinate transformation which commutes with the Hamiltonian
constitutes a symmetry element. Simultaneous translation of all real
particles corresponds to momentum of the center of mass and is of no concern
for the internal Hamiltonian. Also, as stated above, rotational symmetry is
specified by the restriction of $\phi _k$ to $J=0$ angular momentum
eigenstates. Another important symmetry of the Hamiltonian, permutational
symmetry, arises when particles are in some sense indistinguishable or
conjugate. Indistinguishability arises when particles possess the same
masses and charges and is usually an obvious symmetry of the Hamiltonian.
Less obvious permutational symmetries also occur, as in the case of Ps$_2$
where simultaneous charge reversal of the positrons and electrons is a
permutational symmetry of the Hamiltonian (see below). The handling of
permutational symmetry for explicitly correlated Gaussians is outlined
briefly in what follows.

\subsection{Induced symmetry transformations}

Elementary permutations of real particles induce transformations on pseudo
particles. Let $P$ be a permutation of real particles, then, under the
center of mass transformation (\ref{tranT}) the permutation $P$ induces the
transformation $TPT^{-1}\otimes I_3$ on center of mass and internal
coordinates. Further, since $P$ interchanges particles with the same mass
and thus leaves the center of mass unchanged, it follows that $%
TPT^{-1}\otimes I_3$ is a direct sum of the identity transformation on the
center of mass and an induced transformation on internal coordinates 
\begin{equation}
TPT^{-1}\otimes I_3=\left( I_1\oplus \tau _P\right) \otimes I_3=I_3\oplus
\left( \tau _P\otimes I_3\right)  \label{TPT}
\end{equation}

In the special case of atoms, when the natural choice for particle one is
the nucleus, the $\tau _P$ are elementary permutations on pseudo particles.
For example, the $\tau _P$ for He are the $2\times 2$ permutation matrices
for the symmetric group $S_2$ and for Be the $\tau _P$ are the 24 $4\times 4$
permutation matrices for $S_4$.

In the case of Ps$_2$ the $\tau _P$ are not elementary permutations. For Ps$%
_2$ the permutational symmetry elements make up a group of order 8
consisting of: E, the identity; (12), interchange of positrons; (34),
interchange of electrons; (12)(34) interchange of positrons and interchange
of electrons; (13)(24) and (14)(23), charge reversal of class one; (1324)
and (1423), charge reversal of class two. Now, as an example, $\tau _{(12)}$
is derived from the permutation 
\begin{equation}
P_{(12)}=\left( 
\begin{array}{cccc}
0 & 1 & 0 & 0 \\ 
1 & 0 & 0 & 0 \\ 
0 & 0 & 1 & 0 \\ 
0 & 0 & 0 & 1
\end{array}
\right)
\end{equation}
by the transformation 
\begin{equation}
TP_{\left( 12\right) }T^{-1}\otimes I_3=\left[ 
\begin{array}{cccc}
1 & 0 & 0 & 0 \\ 
0 & -1 & 0 & 0 \\ 
0 & -1 & 1 & 0 \\ 
0 & -1 & 0 & 1
\end{array}
\right] \otimes I_3=I_3\oplus \left( \tau _{\left( 12\right) }\otimes
I_3\right)
\end{equation}
Thus, 
\begin{equation}
\tau _{\left( 12\right) }=\left[ 
\begin{array}{ccc}
-1 & 0 & 0 \\ 
-1 & 1 & 0 \\ 
-1 & 0 & 1
\end{array}
\right]
\end{equation}
All of the induced transformations for Ps$_2$ along with a complete
discussion of the permutational symmetry of Ps$_2$ is presented in Kinghorn93%
\cite{Kinghorn93}. Of course the permutation $P$ transforms the basis
functions by 
\begin{equation}
P\phi _l=\exp \left[ -r^{\prime }\left( \tau _P^{\prime }A_l\tau _P\otimes
I_3\right) r\right]
\end{equation}

\subsection{Symmetry projectors}

Symmetry projectors on real particles are transformed to projectors on
pseudo particles using the transformation (\ref{TPT}), thus, 
\begin{equation}
\mathcal{P}=\sum_P\chi _PP\,\,\mapsto \,\,\sum_P\chi _P\tau _P
\end{equation}
Hence, $\mathcal{P}$ acts on $\phi _l$ as 
\begin{equation}
\mathcal{P}\phi _l=\sum_P\chi _P\exp \left[ -r^{\prime }\left( \tau
_P^{\prime }A_l\tau _P\otimes I_3\right) r\right]
\end{equation}

Energy eigenstates of the internal Hamiltonian transform irreducibly under
the induced pseudo particle symmetry group. Symmetry projectors for the
ground state energy eigenstates of He, Be, and Ps$_2$ are now presented.

For He the internal Hamiltonian is invariant under the group consisting of
the identity operation E, and the interchange of the two electrons, (12).
The projector for the ground state of He is thus, 
\begin{equation}
\mathcal{P}^{He}=E+\left( 12\right)  \label{Heproj}
\end{equation}

For Be, using the representation theory of the symmetric group and the spin
free formalism\cite{Matson70}, the ground state can be labeled by the Young
tableaux shape $\boxplus $. The corresponding standard Young tableaux for
this 2-dimensional irreducible representation of $S_4$ are 
\[
T_1= 
\begin{tabular}{|l|l|}
\hline
1 & 2 \\ \hline
3 & 4 \\ \hline
\end{tabular}
\,\,\,\,\,\,\,\,\,\,\,\,T_2= 
\begin{tabular}{|l|l|}
\hline
1 & 3 \\ \hline
2 & 4 \\ \hline
\end{tabular}
\]
Two suitable symmetry projectors can be obtained using the Young $E_{11}=PN$
and $E_{12}=PN(23)$operators 
\begin{equation}
E_{11}=PN=\left[ \left( E+\left( 12\right) \right) \left( E+\left( 34\right)
\right) \right] \left[ \left( E-\left( 13\right) \right) \left( E-\left(
24\right) \right) \right]  \label{NP}
\end{equation}
and 
\begin{equation}
E_{12}=PN\left( 23\right) =\left[ \left( E+\left( 12\right) \right) \left(
E+\left( 34\right) \right) \right] \left[ \left( E-\left( 13\right) \right)
\left( \left( 23\right) -\left( 34\right) \right) \right]
\end{equation}
The two independent projectors arise because this a two dimensional
irreducible representation. In the spin formalism these two states
correspond to the two singlet spin eigenstates for four spin 1/2 particles.
Presumably the best trial function would be a weighted combination of both
projectors on each basis function, $\left( E_{11}+b_kE_{12}\right) \phi _k$,
however, using only one of the projector will produce a trial function with
the correct symmetry properties. Therefore, only the eigenstate projected
using $E_{11}$ will be used in this work.

The permutational symmetry group for Ps$_2$ is isomorphic with the point
group D$_{2d}$, therefore, characters and irreducible representations for D$%
_{2d}$ are used to derive the projector of the ground state of Ps$_2$ an A$%
_1 $ labeled state, 
\begin{equation}
\mathcal{P}^{Ps_2}=E+\left( 12\right) +\left( 34\right) +\left( 12\right)
\left( 34\right) +\left( 1324\right) +\left( 1423\right) +\left( 13\right)
\left( 24\right) +\left( 14\right) \left( 23\right)  \label{Ps2proj}
\end{equation}

Computational effort for computing matrix elements with symmetry projected
basis functions can be reduced by a factor the order of the group by
exploiting commutation of the symmetry projectors with the Hamiltonian and
identity operators. In general 
\begin{equation}
\left\langle \mathcal{P}\phi _k\right| H\left| \mathcal{P}\phi
_l\right\rangle =\left\langle \phi _k\right| H\left| \mathcal{P}^{\dagger }%
\mathcal{P}\phi _l\right\rangle
\end{equation}
Thus, symmetry projection need only be performed on the ket. The projection
operators defined above for He and Ps$_2$ are hermitian and idempotent,
thus, $\mathcal{P}^{\dagger }=\mathcal{P}$ and $\mathcal{P}\mathcal{P}=%
\mathcal{P}.$ However, the Young $PN$ operator $E_{11}$ is not idempotent,
although, the $P$ and $N$ operators are essentially idempotent i.e. $PP=4P$.
A ket projector for Be is given by, 
\begin{equation}
\mathcal{P}^{Be}=\left( E_{11}\right) ^{\dagger }E_{11}=NPPN=4NPN
\end{equation}
Thus, 
\begin{eqnarray}
\mathcal{P}^{Be} &=&16E+8\left( 12\right) -16\left( 13\right) +8\left(
14\right) +8\left( 23\right) -16\left( 24\right) +  \nonumber  \label{Beproj}
\\
&&8\left( 34\right) +16\left( 12\right) \left( 34\right) +16\left( 13\right)
\left( 24\right) +16\left( 14\right) \left( 23\right) -8\left( 123\right) - 
\nonumber \\
&&8\left( 132\right) -8\left( 124\right) -8\left( 142\right) -8\left(
134\right) -8\left( 143\right) -  \label{Beproj} \\
&&8\left( 234\right) -8\left( 243\right) -16\left( 1234\right) +8\left(
1243\right) +8\left( 1324\right) +  \nonumber \\
&&8\left( 1342\right) +8\left( 1423\right) -16\left( 1432\right)  \nonumber
\end{eqnarray}
This is the ket projector used for Be in the calculations.

In every case the ket projector assumes the form 
\begin{equation}
\mathcal{P=}\sum_P\chi _PP
\end{equation}
where the numerical coefficients $\chi _P$ (not to be confused with
character) appear in equations \ref{Heproj}, \ref{Ps2proj}, and \ref{Beproj}
for He, Ps$_2$ and Be respectively.

\section{Integrals for Symmetry Adapted Basis Functions}

In paper 1 integral formulas for non-symmetry projected basis functions were
derived in detail. Here the modifications needed for integrals when symmetry
projection is applied to kets is presented, along with a few hints on how to
implement the formulas efficiently. The matrix elements needed are of the
form 
\[
O_{kl}=\left\langle \phi _k\right| O\left| \mathcal{P}\phi _k\right\rangle
=\sum_P\chi _P\left\langle \phi _k\right| O\left| \exp \left[ -r^{\prime
}\left( \tau _P^{\prime }A_l\tau _P\otimes I_3\right) \right] \right\rangle 
\]
where $O$ is the identity operator, the kinetic energy operator or the
potential energy operator.

Let $A_{kl}=A_k+\tau _P^{\prime }A_l\tau _P$ then the overlap matrix element
is defined as,

\begin{equation}
S_{kl}=\frac{1/f\,\,\left\langle \phi _k\right. |\left. \mathcal{P}\phi
_l\right\rangle }{\left( \left\langle \phi _k\right. |\left. \phi
_k\right\rangle \left\langle \phi _l\right. |\left. \phi _l\right\rangle
\right) ^{1/2}}=\sum_P^f\chi _P\frac{2^{3n/2}}f\left( \frac{\left\|
L_k\right\| \left\| L_l\right\| }{\left| A_{kl}\right| }\right) ^{3/2}
\label{skl}
\end{equation}
where $f$ is the number of terms in the projector, introduced for scaling.
Note that the symmetry projection is not performed on the normalization
factors, this is chosen for computational convenience, since the choice of
normalization has no effect on the calculations other than as matrix
preconditioning for numerical stability.

Kinetic energy matrix elements are given by,

\begin{equation}
T_{kl}=6S_{kl}\sum_P^f\chi _P\,\mathrm{tr}\,\left[ MA_kA_{kl}^{-1}\tau
_P^{\prime }A_l\tau _P\right]  \label{Tkl}
\end{equation}
This formula is implemented efficiently by utilizing $\,\mathrm{tr}\,\left[
AB\right] =\sum_{i,j}\left( A\odot B^{\prime }\right) _{i,j}$. That is, form
the Hadamard, (term by term), product of $A$ and $B$ transpose, and then add
up all of the components of the product.

Potential energy matrix elements have the form,

\begin{equation}
V_{kl}=\frac 2{\sqrt{\pi }}S_{kl}\sum_P^f\chi _P\left( \,\mathrm{\,%
\QTR{mathrm}{vech}\,}\left[ \,Q^{\prime }\right] \right) ^{\prime }\left( 
\mathcal{L}_n\mathcal{R}_n\,\mathrm{vec}\left[ \,A_{kl}^{-1}\right] \right)
^{\left[ -1/2\right] }  \label{Vkl}
\end{equation}
This formula may at first look difficult to implement, however, it is
actually very easy to compute using the following identity, For any
symmetric matrix $A$ , ($A_{kl}^{-1}$ is symmetric). 
\begin{equation}
\left( \mathcal{L}_n\mathcal{R}_n\,\mathrm{vec}\left[ \,A\right] \right)
^{\left[ -1/2\right] }=\,\mathrm{\,\QTR{mathrm}{vech}\,}\left[ \,\left(
\left\{ 
\begin{array}{ll}
\left( a_{ii}\right) ^{-1/2} & i=j \\ 
\left( a_{ii}+a_{jj}-2a_{ij}\right) ^{-1/2} & i>j
\end{array}
\right. \right) \right]
\end{equation}

\section{Gradient}

In this section the effects of symmetry projection on the components of the
energy gradient are given along with a few implementation hints. For a
complete derivation of the energy gradient see the paper 1.

The secular equation 
\begin{equation}
\left( H-\epsilon S\right) c=0
\end{equation}
defines the energy $\epsilon $ as an implicit function of the $N\times N$
matrices $H$ and $S$. $H$ and $S$ are themselves functions of the $Nn\left(
n+1\right) /2\times 1$ vector $a=\left[ \left( \mathrm{vech}\,L_1\right)
^{\prime },\cdots ,\left( \mathrm{vech}\,L_N\right) ^{\prime }\right] $ of
nonlinear exponential parameters contained in the matrices $L_k,$ $^{\prime
} $ recall that $A_k=L_kL_k^{\prime }$. The energy gradient with respect to $%
a$ is then 
\begin{equation}
g=\nabla _a\epsilon =\frac 1{c^{\prime }Sc}\left( \frac{\partial \,\mathrm{%
vech}\,H}{\partial a^{\prime }}-\epsilon \frac{\partial \,\mathrm{vech}\,S}{%
\partial a^{\prime }}\right) ^{\prime }\left( \,\mathrm{vech}\,\left[
2cc^{\prime }-\,\mathrm{diag}\,cc^{\prime }\right] \right)  \label{grad}
\end{equation}

The matrix $\left( \partial \,\mathrm{vech}\,H/\partial a^{\prime }-\epsilon
\,\,\partial \,\mathrm{vech}\,S/\partial a^{\prime }\right) =\partial \,%
\mathrm{vech}\,\left( H-\epsilon S\right) /\partial a^{\prime }$ in the
gradient above is sparse and has dimension $N\left( N+1\right) /2\times
Nn\left( n+1\right) /2$. However this sparse matrix together with the
eigenvector $c$ can be written in the dense partitioned form, 
\begin{equation}
G=\left[ 
\begin{array}{cccc}
c_1^2\frac{\partial \left( H-\epsilon S\right) _{11}}{\partial \left( \,%
\mathrm{vech}\,L_1\right) ^{\prime }} & 2c_1c_2\frac{\partial \left(
H-\epsilon S\right) _{12}}{\partial \left( \,\mathrm{vech}\,L_2\right)
^{\prime }} & \cdots & 2c_1c_N\frac{\partial \left( H-\epsilon S\right) _{1N}%
}{\partial \left( \,\mathrm{vech}\,L_N\right) ^{\prime }} \\ 
2c_2c_1\frac{\partial \left( H-\epsilon S\right) _{21}}{\partial \left( \,%
\mathrm{vech}\,L_1\right) ^{\prime }} & c_2^2\frac{\partial \left(
H-\epsilon S\right) _{22}}{\partial \left( \,\mathrm{vech}\,L_2\right)
^{\prime }} & \cdots & 2c_2c_N\frac{\partial \left( H-\epsilon S\right) _{2N}%
}{\partial \left( \,\mathrm{vech}\,L_N\right) ^{\prime }} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
2c_Nc_1\frac{\partial \left( H-\epsilon S\right) _{N1}}{\partial \left( \,%
\mathrm{vech}\,L_1\right) ^{\prime }} & 2c_Nc_2\frac{\partial \left(
H-\epsilon S\right) _{N2}}{\partial \left( \,\mathrm{vech}\,L_2\right)
^{\prime }} & \cdots & c_N^2\frac{\partial \left( H-\epsilon S\right) _{NN}}{%
\partial \left( \,\mathrm{vech}\,L_N\right) ^{\prime }}
\end{array}
\right]
\end{equation}
With $G$ defined as above the gradient can be computed by summing over the
rows of $G$. That is 
\[
g_i=\frac 1{c^{\prime }Sc}\sum_jG_{ji} 
\]

The non-zero terms in the matrices $\partial \,\mathrm{vech}\,H/\partial
a^{\prime }$ and $\partial \,\mathrm{vech}\,S/\partial a^{\prime }$ are
contained in the $1\times n\left( n+1\right) /2$ vectors $\partial
\,H_{kl}/\partial \left( \,\mathrm{vech}\,L_k\right) ^{\prime }$ and $%
\partial \,S_{kl}/\partial \left( \,\mathrm{vech}\,L_k\right) ^{\prime }$
also\\$\partial \,H_{kl}/\partial \left( \,\mathrm{vech}\,L_l\right)
^{\prime }$ and $\partial \,S_{kl}/\partial \left( \,\mathrm{vech}%
\,L_l\right) ^{\prime }$. Matrix derivatives depend on the order of elements
in a matrix variable, therefore, since symmetry projection on kets
effectively reorders elements of the exponent matrix $L_l$ the formulas for
derivatives with respect to exponent matrices in the ket, $\,\mathrm{vech}%
\,\left[ L_l\right] ,$ are different than those with respect to exponent
matrices in the bra,$\,\mathrm{vech}\,\left[ L_k\right] $. ( Note that $\tau
_P^{\prime }A_l\tau _P=\tau _P^{\prime }L_lL_l^{\prime }\tau _P=\tau
_P^{\prime }L_l\left( \tau _P^{\prime }L_l\right) ^{\prime }$ ). Also, the
matrix derivatives for the diagonal blocks of $G$ are complicated somewhat
by the symmetry projection on the kets. However, they can be computed using
the following relationship, for example, 
\begin{equation}
\frac{\partial H_{kk}}{\partial \left( \,\mathrm{vech}\,L_k\right) ^{\prime }%
}=\left. \frac{\partial H_{kl}}{\partial \left( \,\mathrm{vech}\,L_k\right)
^{\prime }}\right| _{l=k}+\left. \frac{\partial H_{kl}}{\partial \left( \,%
\mathrm{vech}\,L_l\right) ^{\prime }}\right| _{l=k}
\end{equation}
Thus, only two sets of formulas for the derivatives need be computed. These
formulas are now presented. Note: only one term in the symmetry projection
will be represented, and labeled with a superscript $P$. 
\begin{equation}
\text{Define: }A_{kl}=A_k+\tau _P^{\prime }A_l\tau _P
\end{equation}

\subsection{Overlap derivatives}

\begin{equation}
\frac{\partial S_{kl}^P}{\partial \left( \,\mathrm{vech}\,L_k\right)
^{\prime }}=\frac 32S_{kl}^P\left( \,\mathrm{vech}\,\left[ \left( \,\mathrm{%
diag}\,L_k\right) ^{-1}-2A_{kl}^{-1}L_k\right] \right)
\end{equation}

\begin{equation}
\frac{\partial S_{kl}^P}{\partial \left( \,\mathrm{vech}\,L_l\right)
^{\prime }}=\frac 32S_{kl}^P\left( \,\mathrm{vech}\,\left[ \left( \,\mathrm{%
diag}\,L_l\right) ^{-1}-2\tau _PA_{kl}^{-1}\tau _P^{\prime }L_l\right]
\right) ^{\prime }
\end{equation}
Note: in paper 1 the term $\,\mathrm{vech}\,\left( L_l^{-1}\right) ^{\prime
} $ appeared, here the simplification $\,\mathrm{vech}\,\left(
L_l^{-1}\right) ^{\prime }=\,\mathrm{vech}\,\left[ \left( \,\mathrm{diag}%
\,L_k\right) ^{-1}\right] $ is used.

\subsection{Kinetic energy derivatives}

Let's start with a simplifying definition, Define: $A_l^{*}=\tau _P^{\prime
}A_l\tau _P$ Then,

\begin{eqnarray}
\frac{\partial T_{kl}^P}{\partial \left( \,\mathrm{vech}\,L_k\right)
^{\prime }} &=&\left( \,\mathrm{vech}\,\left[ \frac 32T_{kl}^P\left( \left(
\,\mathrm{diag}\,L_k\right) ^{-1}-2A_{kl}^{-1}L_k\right) + 
\begin{array}{c}
\end{array}
\right. \right.  \nonumber \\
&&\left. \left. 
\begin{array}{c}
\end{array}
12S_{kl}^P\left( A_{kl}^{-1}A_l^{*}MA_l^{*}A_{kl}^{-1}L_k\right) \right]
\right) ^{\prime }
\end{eqnarray}
and 
\begin{eqnarray}
\frac{\partial T_{kl}^P}{\partial \left( \,\mathrm{vech}\,L_l\right)
^{\prime }} &=&\,\left( \mathrm{vech}\,\left[ \frac 32T_{kl}^P\left( \left(
\,\mathrm{diag}\,L_l\right) ^{-1}-2\tau _PA_{kl}^{-1}\tau _P^{\prime
}L_l\right) + 
\begin{array}{c}
\end{array}
\right. \right.  \nonumber \\
&&\left. \left. 
\begin{array}{c}
\end{array}
12S_{kl}^P\left( \tau _PA_{kl}^{-1}A_kMA_kA_{kl}^{-1}\tau _P^{\prime
}L_l\right) \right] \right) ^{\prime }
\end{eqnarray}

\subsection{Potential Energy}

The potential energy gradient components are given by,

\begin{eqnarray}
\frac{\partial V_{kl}^P}{\partial \left( \,\,\mathrm{vech}\,L_k\right)
^{\prime }} &=&\,\left( \mathrm{vech}\left[ \frac 32V_{kl}^P\left( \left( \,%
\mathrm{diag}\,L_k\right) ^{-1}-2A_{kl}^{-1}L_k\right) + 
\begin{array}{c}
\end{array}
\right. \right.  \nonumber \\
&&\left. \left. 
\begin{array}{c}
\end{array}
\frac 2{\sqrt{\pi }}S_{kl}^P\left(
\,\,A_{kl}^{-1}D_{kl}A_{kl}^{-1}L_k\right) \right] \right) ^{\prime }
\end{eqnarray}
\begin{eqnarray}
\frac{\partial V_{kl}^P}{\partial \left( \,\,\mathrm{vech}\,L_l\right)
^{\prime }} &=&\,\left( \mathrm{vech}\left[ \frac 32V_{kl}^P\left( \left( \,%
\mathrm{diag}\,L_l\right) ^{-1}-2\tau _PA_{kl}^{-1}\tau _P^{\prime
}L_l\right) + 
\begin{array}{c}
\end{array}
\right. \right.  \nonumber \\
&&\left. \left. 
\begin{array}{c}
\end{array}
\frac 2{\sqrt{\pi }}S_{kl}^P\left( \tau
_P\,\,A_{kl}^{-1}D_{kl}A_{kl}^{-1}\tau _P^{\prime }L_l\right) \right]
\right) ^{\prime }
\end{eqnarray}
Introduction of the matrix $D_{kl}$ gives a significant simplification over
the result presented in the paper 1. If we define $\bar{Q}=Q+Q^{\prime }-\,%
\mathrm{diag}\,Q$ the matrix $D_{kl}$ is given by, 
\begin{equation}
D_{kl}=\left( \left\{ 
\begin{array}{ll}
\left( D_{kl}\right) _{ij}=\sum_{r=1}^n\bar{Q}_{ir}B_{ir}^{-3/2} & i=j \\ 
\left( D_{kl}\right) _{ij}=-\bar{Q}_{ij}B_{ij}^{-3/2} & i\neq j
\end{array}
\right. \right)
\end{equation}
where the matrix $B$ is given by 
\begin{equation}
B=\left( \left\{ 
\begin{array}{ll}
B_{ij}=\left( A_{kl}^{-1}\right) _{ii} & i=j \\ 
B_{ij}=\left( A_{kl}^{-1}\right) _{ii}+\left( A_{kl}^{-1}\right)
_{jj}-2\left( A_{kl}^{-1}\right) _{ij} & i\neq j
\end{array}
\right. \right)
\end{equation}
Algorithms for $D_{kl}$ are relatively simple to implement. This use of $%
D_{kl}$ allows the following substitution to be made in the formula
(equation 134) for the potential energy derivative given in the paper 1. 
\begin{equation}
\mathrm{vech}\left[ \,A_{kl}^{-1}D_{kl}A_{kl}^{-1}L_k\right] =\left( \mathrm{%
vec}Q^{\prime }\odot \left( \mathcal{R}_n\,\mathrm{vec}A_{kl}^{-1}\right)
^{\left[ -3/2\right] }\right) ^{\prime }\mathcal{R}_n\left(
A_{kl}^{-1}L_k\otimes A_{kl}^{-1}\right) \mathcal{L}_n^{\prime }
\end{equation}

\section{Methods}

\subsection{Systems}

The systems investigated in this work are ground states of, $^4$He, $^\infty 
$He, Ps$_2$, $^9$Be, and $^\infty $Be. For the atomic systems He and Be
calculations using both the common isotopic masses and the infinite nuclear
mass approximation were performed. The calculations with the infinite
nuclear mass approximation are included for comparison with other results
from the literature, where calculations using actual isotopic nuclear masses
are rare.

He, the obligatory test system for new methods, is a simple 3 particle
system with only 3 exponent parameters per basis function and only 2
permutations in the symmetry projector. Ps$_2$, a 4 particle system, is a
more challenging test case with 6 parameters per basis function and 8
permutations in the symmetry projector. Meaningful calculations on Ps$_2$
require a non Born-Oppenheimer method with explicit correlation, thus, Ps$_2$
is ideally suited for calculation with explicitly correlated Gaussian basis
functions. Be, with 5 particles is the largest system considered in the
testing. For Be there are 10 parameters per basis function and 24
permutations in the symmetry projector.

The unit of mass is $m_e=1$ yielding energy in hartrees. For Ps$_2$ all
particle masses are all equal to 1. For $^4$He and $^9$Be the nuclear masses
used are from ``The 1983 Atomic Mass Evaluation'' of Wapstra and Audi\cite
{Wapstra}. In units of $m_e=1$ the masses are \{$^4$He: 7296.2947\} and \{$%
^9 $Be: 16428.193\}.

\subsection{Hardware and Software}

Computer programs for the calculations were run on a DEC 2100 4/275. The
programs were written in Fortran 77 and compiled under the OSF/1 operating
system using f77 with the -fast compiler option and linked to dxml (DEC
extended math library) for access to LAPACK\cite{LAPACK} and the BLAS.
LAPACK was used for solving the generalized symmetric eigensystem needed for
the energy calculations. The principle optimization software used was the
package TN written by Stephen Nash\cite{NashTN}. Optimization subroutines
and a gradient test subroutine from the IMSL\cite{IMSL} library where also
used as was code for a Powell direction set method from Numerical Recipes%
\cite{NRinFortran}. Optimization methods will be described in more detail
below. The software package MATLAB\cite{MATLAB} was used heavily for
developing and testing algorithms.

The code for energy and gradient calculations was debugged and tested
thoroughly. Results for matrix elements and energy calculations were
compared against the our previous calculations on Li and Ps$_2$\cite
{Kinghorn93} and by comparison with results obtained by implementing
formulas directly in MATLAB. Thus, two independent checks for all formulas
were obtained. The correctness of the gradient code was verified using the
numerical diagnostics given by the IMSL subroutine DCHGRD and by computing
finite difference approximations to the gradient in Fortran and in MATLAB.

\subsection{Optimization methods}

Four gradient based optimizations methods and one non-gradient method were
tested. It is beyond the scope of this paper to give detailed descriptions
of the optimization methods, thus the reader is referred to the references
for more information. Some useful references include the texts of Dennis and
Schnabel\cite{DennisSchnabel83}, Gill, Murray and Wright\cite
{GillMurrayWright81}, and Luenberger\cite{Luenberger84}. An interesting
monograph by Nazareth\cite{Nazareth94} shows the fundamental unity of the
various methods. Following is a listing of the subroutines used and their
underlying methods.

\begin{itemize}
\item  TN---A truncated Newton package where the quadratic subproblem is
solved approximately using a linear conjugate gradient method, see Nash\cite
{NashTN}. (Source code available from netlib\cite{netlib})

\item  DUMING---A quasi-Newton method utilizing a BFGS update to an
approximated Hessian, see Dennis and Schnabel\cite{DennisSchnabel83}. (IMSL
subroutine)

\item  DUMIDH---A modified Newton method using a finite difference
approximation to the Hessian and model trust-region approach, see Dennis and
Schnabel\cite{DennisSchnabel83} and Gay\cite{Gay83}.(IMSL subroutine)

\item  DUMCGG---A version of the conjugate gradient method described in
Powell\cite{Powell77} using analytic gradients and no Hessian.(IMSL
subroutine)

\item  POWELL---An implementation of a direction set method of Powell that
does not utilize gradient information, see Numerical Recipes\cite
{NRinFortran}.
\end{itemize}

\section{Results}

Implementation of the formulas of paper 1 has resulted in computer code
that, in our experience, represents a substantial improvement in both speed
and accuracy for the calculation of energy bounds using correlated
Gaussians. Our previous computations with this basis set often required days
or weeks of optimization effort, whereas the present implementation
accomplishes the same ends in mere seconds. For small and medium sized basis
sets our energy bounds are better or very close to previously published
results and our 128 term trial functions have given the best energy bounds
so far reported using correlated Gaussian basis functions. Clearly, formulas
of paper 1 perform exceptionally well.

\subsection{Timing results}

It is not our intention to compare different optimizers with one another and
we caution the reader not to draw unwarranted conclusions regarding
superiority of any algorithms. Many factors can effect performance of
non-linear optimization routines; convergence criteria, initial guesses for
parameters, etc.. Also, when choosing optimization software, factors other
than performance way be important, such as, ease of use and availability of
source code.

Table (\ref{Betime}) gives timing data for optimization runs on a 4 term (40
parameter) $^9$Be trial function. The same randomly chosen starting point
was used for each of the optimization runs. The starting sets of exponents
were generated using the randn function in MATLAB which generates normally
distributed set of random numbers with mean 0 and standard deviation 1.
Default stopping criteria were used in each routine. The table reports the
energy in hartrees , the magnitude of the gradient, $\left\| g\right\|
_2=\left( g^{\prime }g\right) ^{1/2}$, the total number of energy and
gradient calls, the number of major iterations for the method and the total
time for the optimization run. Blanks in the tables represent information
that was not provided by the corresponding routine. Note: for the POWELL
routine no gradient information is computed.

\TeXButton{B}{\begin{table}[h] \centering} 
\begin{tabular}{llllll}
\hline\hline
method & energy & $\left\| g\right\| _2$ & calls & iter & tot. t/sec \\ 
\hline
TN & -14.502506 & $2.2\times 10^{-7}$ & 477 & 66 & 19.7 \\ 
DUMING & -14.299668 & $8.5\times 10^{-5}$ & 325 & 149 & 13.2 \\ 
DUMIDH & -14.308297 & $2.3\times 10^{-5}$ & 122 & 42 & 71.6 \\ 
DUMCGG & -14.298264 & $5.4\times 10^{-7}$ & --- & --- & 68.1 \\ 
POWELL & -14.305858 & --- & --- & 71 & 2132.0 \\ \hline\hline
\end{tabular}
\caption{Comparison of optimization methods using a 4 term $ ^{9}$Be trial
function\label{Betime}}\TeXButton{E}{\end{table}}

\TeXButton{B}{\begin{table}[h] \centering} 
\begin{tabular}{llllll}
\hline\hline
method & energy & $\left\| g\right\| _2$ & calls & iter & tot t/sec \\ \hline
TN & -.5133492 & $1.7\times 10^{-7}$ & 1231 & 119 & 44.2 \\ 
DUMING & -.5133492 & $9.3\times 10^{-6}$ & 1052 & 488 & 37.3 \\ 
DUMIDH & -.5133492 & $5.7\times 10^{-6}$ & 183 & 62 & 115.6 \\ 
DUMCGG & -.5133492 & $9.7\times 10^{-8}$ & --- & --- & 285.7 \\ 
POWELL & -.5132153 & --- & --- & 174 & 5402.5 \\ \hline\hline
\end{tabular}
\caption{Comparison of optimization methods using an 8 term Ps$_{2}$ trial
function\label{Pstime}}\TeXButton{E}{\end{table}}

Table (\ref{Betime}) illustrates three general points: 1) the new energy and
gradient code is very fast, 2) the POWELL, non-gradient, routine is more
than an order of magnitude slower than any of the gradient based methods, 3)
non-linear optimization can be unpredictable. The information provided by
the analytic gradient accelerates optimization, in this case by a factor of
160 between the DUMING\ and POWELL routines. The unpredictably of non-linear
optimization is illustrated by the difference in energy results. We
speculate that the TN run has converged to a global minimum while the others
have converged to some other local minimum, even though all runs were
started from the same initial point.

Table (\ref{Pstime}) applies to an 8 term (48 parameter) trial function for
Ps$_2$ and illustrates the same speed benefits of gradient methods. But, in
this case all five routines are apparently reaching the same minimum.

Table (\ref{egtime}) shows how the energy and gradient calculation scales
with increased system size and with increased basis size. The time increase
with system size reflects the effect of increasing the terms in the symmetry
projector, 2, 8, 24, respectively for He, Ps$_2$, and Be, and the size of
the exponent matrices, $2\times 2,\;3\times 3$ and $4\times 4$, respectively.

\TeXButton{B}{\begin{table}[h] \centering} 
\begin{tabular}{lllllll}
\hline\hline
N & 4 & 8 & 16 & 32 & 64 & 128 \\ \hline
He & .01 & .02 & .04 & .14 & .54 & 2.0 \\ 
Ps$_2$ & .02 & .05 & .16 & .62 & 2.2 & 9.2 \\ 
Be & .05 & .17 & .57 & 2.1 & 8.5 & 34.7 \\ \hline\hline
\end{tabular}
\caption{Time, in seconds, for 1 energy and gradient evaluation for
 He, Ps$_{2}$, Be using from 4 to 128 trial functions\label{egtime}}%
\TeXButton{E}{\end{table}}

These timing tests on three diverse systems are encouraging. Further
improvements are certainly possible. More experimentation with optimization
methods and fine tuning of the methods would likely improve the results.
However, we believe such refinements will make a smaller impact than our
newly developed formulas for matrix elements and gradient which have made it
possible to exploit the power of optimization subroutines such as TN.

\subsection{Energy calculations}

Tables (\ref{He4eng}), (\ref{Heeng}), (\ref{Ps2eng}), (\ref{Be9eng}), and (%
\ref{Beeng}) present results for ground state energy calculations on $^4$He,
He, Ps$_2$, $^9$Be, and Be respectively. TN was used for the optimization in
all of these calculations. Default convergence criteria were used. The
initial guesses for the exponents for $^4$He, Ps$_2$, and $^9$Be were random
numbers generated using the randn command from MATLAB. The initial guesses
for exponents of $^\infty $He and $^\infty $Be were obtained from the
optimized points for $^4$He and $^9$Be and required few additional
iterations. The values reported in the tables are: the energy in hartrees,
the magnitude of the gradient, $\left\| g\right\| _2$, and the absolute
value of one minus the virial coefficient, $\left| 1-\eta \right| $. The
virial coefficient $\eta =-\left\langle V\right\rangle /2\left\langle
T\right\rangle $ was computed as a simple check on the quality of the wave
function. Energy results are rounded to the nearest microhartree except for
Ps$_2$ where results are rounded to a tenth of a microhartree.

Although these energy calculations were done primarily to test the new
analytic gradient formulas they are actually excellent results in their own
right. These are the best results yet reported for these systems using this
basis set. However, it is likely that these results can be improved, and
future work should bare this out.

The 128 term $^4$He and $^\infty $He energy results are on the order of one
wave number away from the exact results and gradient magnitudes on the order
of $10^{-6}$ suggest that further optimization progress may be possible.

\TeXButton{B}{\begin{table}[tbp] \centering} 
\begin{tabular}{llll}
\hline\hline
N & energy & $\left\| g\right\| _2$ & $\left| 1-\eta \right| $ \\ \hline
4 & -2.873276 & $7.7\times 10^{-9}$ & $3.0\times 10^{-9}$ \\ 
8 & -2.899000 & $9.4\times 10^{-8}$ & $7.0\times 10^{-9}$ \\ 
16 & -2.902915 & $2.7\times 10^{-7}$ & $1.0\times 10^{-9}$ \\ 
32 & -2.903280 & $3.3\times 10^{-7}$ & $1.5\times 10^{-7}$ \\ 
64 & -2.903296 & $7.5\times 10^{-6}$ & $3.8\times 10^{-7}$ \\ 
128 & -2.903302 & $3.7\times 10^{-6}$ & $3.5\times 10^{-7}$ \\ 
exact\cite{Kleidienst81} & -2.90330769 &  &  \\ \hline\hline
\end{tabular}
\caption{$ ^{4}$He energy calculations using from 4 to 128 basis functions
 Optimization using TN\label{He4eng}}\TeXButton{E}{\end{table}}

\TeXButton{B}{\begin{table}[tbp] \centering} 
\begin{tabular}{llll}
\hline\hline
N & energy & $\left\| g\right\| _2$ & $\left| 1-\eta \right| $ \\ \hline
4 & -2.873692 & $8.6\times 10^{-8}$ & $7.0\times 10^{-9}$ \\ 
8 & -2.899420 & $1.2\times 10^{-7}$ & $2.8\times 10^{-8}$ \\ 
16 & -2.903466 & $1.9\times 10^{-7}$ & $2.9\times 10^{-9}$ \\ 
32 & -2.903700 & $4.0\times 10^{-7}$ & $9.1\times 10^{-8}$ \\ 
64 & -2.903716 & $5.6\times 10^{-6}$ & $1.1\times 10^{-6}$ \\ 
128 & -2.903722 & $2.3\times 10^{-6}$ & $3.3\times 10^{-7}$ \\ 
exact\cite{Pekeris60} & -2.903724376 &  &  \\ \hline\hline
\end{tabular}
\caption{$ ^{\infty}$He energy calculations using from 4 to 128 basis functions
 Optimization using TN\label{Heeng}}\TeXButton{E}{\end{table}}

The energy result -.51599796 for Ps$_2$ establishes a new upper bound on the
ground state energy of this system and is approximately 20 micro-hartrees
lower in energy than previously reported results\cite{Kozlowski93b},\cite
{Kinghorn93}.

\TeXButton{B}{\begin{table}[tbp] \centering} 
\begin{tabular}{llll}
\hline\hline
N & energy/hartree & $\left\| g\right\| _2$ & $\left| 1-\eta \right| $ \\ 
\hline
4 & -.5062911 & $2.9\times 10^{-8}$ & $8.0\times 10^{-9}$ \\ 
8 & -.5133492 & $1.7\times 10^{-7}$ & $2.0\times 10^{-9}$ \\ 
16 & -.5154558 & $1.5\times 10^{-7}$ & $7.2\times 10^{-9}$ \\ 
32 & -.5158766 & $3.9\times 10^{-7}$ & $2.0\times 10^{-8}$ \\ 
64 & -.5159844 & $5.1\times 10^{-7}$ & $1.9\times 10^{-7}$ \\ 
128 & -.51599796 & $2.2\times 10^{-6}$ & $1.8\times 10^{-6}$ \\ \hline\hline
\end{tabular}
\caption{Ps$_{2}$ energy calculations using from 4 to 128 basis functions
 Optimization using TN\label{Ps2eng}}\TeXButton{E}{\end{table}}

No other calculations for the ground state energy of $^9$Be were found in
the literature, however, our results suggest that the ground state energy of
this system is approximately one millihartree above the infinite nuclear
mass limit. The exact result for $^\infty $Be reported in table (\ref{Beeng}%
) is the non-relativistic estimate of Bunge\cite{Bunge76} the actual
rigorous upper bound computed by Bunge is -14.666902E$_h$ which is very
close the result of Clementi, -14.6669598E$_h.$ The best $^\infty $Be result
using correlated Gaussian basis functions prior to the present work is that
of Schwegler\cite{Schwegler93} where a ground state energy of -14.662834E$_h$
was reported using a 150 term wave function which was only partially
optimized. Gradient magnitudes on the order of $10^{-3}$ indicate that
further optimization progress is possible for the Be results presented here.

\TeXButton{B}{\begin{table}[tbp] \centering} 
\begin{tabular}{llll}
\hline\hline
N & energy/hartree & $\left\| g\right\| _2$ & $\left| 1-\eta \right| $ \\ 
\hline
4 & -14.502506 & $2.2\times 10^{-7}$ & $9.0\times 10^{-9}$ \\ 
8 & -14.603063 & $2.7\times 10^{-7}$ & $7.0\times 10^{-9}$ \\ 
16 & -14.640292 & $7.1\times 10^{-7}$ & $8.0\times 10^{-9}$ \\ 
32 & -14.655832 & $2.7\times 10^{-5}$ & $3.8\times 10^{-7}$ \\ 
64 & -14.663941 & $5.5\times 10^{-6}$ & $5.9\times 10^{-8}$ \\ 
128 & -14.664395 & $1.9\times 10^{-3}$ & $1.3\times 10^{-5}$ \\ \hline\hline
\end{tabular}
\caption{$ ^{9}$Be energy calculations using from 4 to 128 basis functions
 Optimization using TN\label{Be9eng}}\TeXButton{E}{\end{table}}

\TeXButton{B}{\begin{table}[tbp] \centering} 
\begin{tabular}{llll}
\hline\hline
N & energy/hartree & $\left\| g\right\| _2$ & $\left| 1-\eta \right| $ \\ 
\hline
4 & -14.503421 & $1.5\times 10^{-7}$ & $5.8\times 10^{-10}$ \\ 
8 & -14.603984 & $2.8\times 10^{-7}$ & $7.0\times 10^{-9}$ \\ 
16 & -14.641214 & $1.3\times 10^{-6}$ & $2.1\times 10^{-8}$ \\ 
32 & -14.656012 & $1.2\times 10^{-6}$ & $5.0\times 10^{-9}$ \\ 
64 & -14.664862 & $5.4\times 10^{-6}$ & $6.9\times 10^{-8}$ \\ 
128 & -14.665397 & $1.8\times 10^{-3}$ & $2.9\times 10^{-5}$ \\ 
exact\cite{Bunge76} & -14.667358 &  &  \\ \hline\hline
\end{tabular}
\caption{$ ^{\infty}$Be energy calculations using from 4 to 128 basis functions
 Optimization using TN\label{Beeng}}\TeXButton{E}{\end{table}}

\section{Conclusions}

The formulas for matrix elements and analytic gradients in paper 1 are
expressed in compact matrix notation made possible by the modern calculus of
matrix variables. The notation makes the computer codes succinct, efficient,
and easily debugged. The programming process is virtually painless when
compared to previous formulas. The new formulas and computer code constitute
a major development in the application of correlated Gaussian basis
functions.

An efficient computer program for analytic gradients significantly reduces
the optimization effort inherent in the correlated Gaussian basis. Much more
completely optimized wave functions are now possible. There remain
additional problems that hamper exploitation of the correlated Gaussian
basis functions. Some of these are rather elementary and require only
careful application of known mathematical techniques: higher angular
momentum states (paper 1 is limited to J=0), multi-center correlated
Gaussians natural to Born-Oppenheimer wave functions of polyatomic
molecules. Other difficulties appear to be largely insoluble except by shear
brute strength, e.g. the vast number of permutations required to project a
Fermi allowed state from an n-particle basis function.

In the case of Ps$_2$ our newly computed upper bound supercedes all previous
variational bounds. Notably, the current results for this system using only
128 basis functions are below those previously achieved using 300 basis
functions\cite{Kinghorn93}. Our present result for the $^4$He ($^\infty $He)
atom using 128 correlated Gaussians is only 2 (5) microhartrees above the
nearly exact Hylleraas treatment. For $^9$Be ($^\infty $Be) our 128 basis
function results are still over a millihartree above the estimated ground
state but are several millihartrees below the best previously reported
results using this basis set. 98\% of the correlation energy has been
recovered for Be using 128 basis functions\cite{Schwegler93}.

The present results confirm our expectation that formulas of paper 1 will
extend the practical range of the correlated Gaussian basis. Our general
computer program is currently being applied to several new systems.

\bibliographystyle{aip}
\bibliography{4-prefs,mcalc}

\appendix 

\chapter{Fortran Code}

This appendix gives a listing of the Fortran code used in the calculations.
The code requires linking to a local BLAS and LAPACK installation. Also, the
optimization code TN of Stephen Nash (see references) or some other
optimization subroutine needs to be included and called from the driver
program, CGAUSS.

MELKL.FOR\ is the subroutine for computing matrix elements and gradient
components. (Well commented).

MELKK.FOR is MELKL.FOR with the name of the subroutine changed and only
slight modification at the end of the file. The added lines of code at the
end of the file are the only line included in this listing.

ENGRAD.FOR is subroutine called by TN to supply the energy and gradient at a
given point X. (Moderately commented)

CGAUSS.FOR is the driver program and is somewhat messy from testing and IO
remnants. However, it is perfectly functional and supplies a template for
building other driver programs.

\section{MELKL.FOR}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{melkl.for} }

\section{MELKK.FOR}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{melkk.for} }

\section{ENGRAD.FOR}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{engrad.for} }

\section{CGAUSS.FOR}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{cgauss.for} }

\chapter{Sample Input Files}

These sample input files (commented) are read in to CGAUSS with file
redirection. For example, cgauss \TEXTsymbol{<} He4.in1. The files listed
include the mass matrix charge vector and symmetry projection data along
with parameters for the optimizer and eigensystem solver.

\section{$^4$He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he2.in1} }

\section{$^\infty $He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he2.ini} }

\section{Ps$_2$}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{ps2.in1} }

\section{$^9$Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be2.in1} }

\section{$^\infty $Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be2.ini} }

\chapter{Wave Functions}

Exponential parameters and linear coefficients for wave functions with up to
16 basis functions are included in this appendix. The wave functions are of
the form 
\[
\psi =\sum_{k=1}^Nc_k\mathcal{P}^\alpha \exp \left[ r^{\prime }\left(
L_kL_k^{\prime }\right) r\right] 
\]
($\mathcal{P}^\alpha $ is the symmetry projector the $\alpha $ system and
state) The data in the following sections is from the optimization output
files. The last iteration of the optimization is given along with the timing
data for the run, followed by the energy and the point X, which is the
vector of exponent parameters $a=\left[ \left( \,\mathrm{vech}\,L_1\right)
^{\prime },\cdots ,\left( \,\mathrm{vech}\,L_N\right) ^{\prime }\right] ,$
the eigenvector containing the linear coefficients is next followed by the
virial coefficient, kinetic and potential energy.

\section{$^4$He}

\subsection{4 term $^4$He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he4.wf} }

\subsection{8 term $^4$He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he8.wf} }

\subsection{16 term $^4$He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he16.wf} }

\section{$^\infty $He}

\subsection{4 term $^\infty $He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he4.wfi} }

\subsection{8 term $^\infty $He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he8.wfi} }

\subsection{16 term $^\infty $He}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{he16.wfi} }

\subsection{32 term $^\infty $He}

\section{Ps$_2$}

\subsection{4 term Ps$_2$}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{ps4.wf} }

\subsection{8 term Ps$_2$}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{ps8.wf} }

\subsection{16 term Ps$_2$}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{ps16.wf} }

\section{$^9$Be}

\subsection{4 term $^9$Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be4.wf} }

\subsection{8 term $^9$Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be8.wf} }

\subsection{16 term $^9$Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be16.wf} }

\section{$^\infty $Be}

\subsection{4 term $^\infty $Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be4.wfi} }

\subsection{8 term $^\infty $Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be8.wfi} }

\subsection{16 term $^\infty $Be}

{\normalsize \renewcommand{\baselinestretch}{1} \footnotesize %
\verbatiminput{be16.wfi} }

\end{document}
